{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning for seq2seq\n",
    "\n",
    "This time we'll solve a problem of transribing hebrew words in english, also known as g2p (grapheme2phoneme)\n",
    "\n",
    " * word (sequence of letters in source language) -> translation (sequence of letters in target language)\n",
    "\n",
    "Unlike what most deep learning practicioners do, we won't only train it to maximize likelihood of correct translation, but also employ reinforcement learning to actually teach it to translate with as few errors as possible.\n",
    "\n",
    "\n",
    "### About the task\n",
    "\n",
    "One notable property of Hebrew is that it's consonant language. That is, there are no wovels in the written language. One could represent wovels with diacritics above consonants, but you don't expect people to do that in everyay life.\n",
    "\n",
    "Therefore, some hebrew characters will correspond to several english letters and others - to none, so we should use encoder-decoder architecture to figure that out.\n",
    "\n",
    "![img](https://esciencegroup.files.wordpress.com/2016/03/seq2seq.jpg)\n",
    "_(img: esciencegroup.files.wordpress.com)_\n",
    "\n",
    "Encoder-decoder architectures are about converting anything to anything, including\n",
    " * Machine translation and spoken dialogue systems\n",
    " * [Image captioning](http://mscoco.org/dataset/#captions-challenge2015) and [image2latex](https://htmlpreview.github.io/?https://github.com/openai/requests-for-research/blob/master/_requests_for_research/im2latex.html) (convolutional encoder, recurrent decoder)\n",
    " * Generating [images by captions](https://arxiv.org/abs/1511.02793) (recurrent encoder, convolutional decoder)\n",
    " * Grapheme2phoneme - convert words to transcripts\n",
    "  \n",
    "We chose simplified __Hebrew->English__ machine translation for words and short phrases (character-level), as it is relatively quick to train even without a gpu cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !wget https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week07_seq2seq/basic_model_torch.py -O basic_model_torch.py\n",
    "    !wget https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week07_seq2seq/main_dataset.txt -O main_dataset.txt\n",
    "    !wget https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week07_seq2seq/voc.py -O voc.py\n",
    "    !pip3 install torch==1.0.0 nltk editdistance\n",
    "\n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week6_outro/seq2seq/submit.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If True, only translates phrases shorter than 20 characters (way easier).\n",
    "EASY_MODE = True\n",
    "# Useful for initial coding.\n",
    "# If false, works with all phrases (please switch to this mode for homework assignment)\n",
    "\n",
    "# way we translate. Either \"he-to-en\" or \"en-to-he\"\n",
    "MODE = \"he-to-en\"\n",
    "# maximal length of _generated_ output, does not affect training\n",
    "MAX_OUTPUT_LENGTH = 50 if not EASY_MODE else 20\n",
    "REPORT_FREQ = 100                          # how often to evaluate validation score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: preprocessing\n",
    "\n",
    "We shall store dataset as a dictionary\n",
    "`{ word1:[translation1,translation2,...], word2:[...],...}`.\n",
    "\n",
    "This is mostly due to the fact that many words have several correct translations.\n",
    "\n",
    "We have implemented this thing for you so that you can focus on more interesting parts.\n",
    "\n",
    "\n",
    "__Attention python2 users!__ You may want to cast everything to unicode later during homework phase, just make sure you do it _everywhere_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size =  130114\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "word_to_translation = defaultdict(list)  # our dictionary\n",
    "\n",
    "bos = '_'\n",
    "eos = ';'\n",
    "\n",
    "with open(\"main_dataset.txt\", encoding=\"utf-8\") as fin:\n",
    "    for line in fin:\n",
    "\n",
    "        en, he = line[:-1].lower().replace(bos, ' ').replace(eos,\n",
    "                                                             ' ').split('\\t')\n",
    "        word, trans = (he, en) if MODE == 'he-to-en' else (en, he)\n",
    "\n",
    "        if len(word) < 3:\n",
    "            continue\n",
    "        if EASY_MODE:\n",
    "            if max(len(word), len(trans)) > 20:\n",
    "                continue\n",
    "\n",
    "        word_to_translation[word].append(trans)\n",
    "\n",
    "print(\"size = \", len(word_to_translation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'akbara\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_translation['עכברה']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['משתמש:צלף/!',\n",
       " 'סימן קריאה',\n",
       " 'תבנית:!!',\n",
       " '$9.99',\n",
       " \"תבנית:'\",\n",
       " 'עבד אל-אילה',\n",
       " 'עבד-אל סטאר קאסם',\n",
       " \"עין ע'זאל\",\n",
       " 'עכברה',\n",
       " 'אלו אלו']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word_to_translation.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all unique lines in source language\n",
    "all_words = np.array(list(word_to_translation.keys()))\n",
    "# get all unique lines in translation language\n",
    "all_translations = np.array(list(set(\n",
    "    [ts for all_ts in word_to_translation.values() for ts in all_ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"seán o'casey\", 'visual cryptography', 'william hood simpson',\n",
       "       'carli lloyd', 'saint-chamond', 'death becomes her', 'el djem',\n",
       "       'mcnichol', '1985 films', 'raimi'], dtype='<U20')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "all_translations[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split the dataset\n",
    "\n",
    "We hold out 10% of all words to be used for validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_words, test_words = train_test_split(\n",
    "    all_words, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building vocabularies\n",
    "\n",
    "We now need to build vocabularies that map strings to token ids and vice versa. We're gonna need these fellas when we feed training data into model or convert output matrices into english words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from voc import Vocab\n",
    "inp_voc = Vocab.from_lines(''.join(all_words), bos=bos, eos=eos, sep='')\n",
    "out_voc = Vocab.from_lines(''.join(all_translations), bos=bos, eos=eos, sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines\n",
      "['משתמש:צלף/!' 'סימן קריאה' 'תבנית:!!' '$9.99' \"תבנית:'\"]\n",
      "\n",
      "words to ids (0 = bos, 1 = eos):\n",
      "[[  0 127 138 139 127 138  27 135 125 132  16   3   1]\n",
      " [  0 130 122 127 128   2 136 137 122 113 117   1   1]\n",
      " [  0 139 114 129 122 139  27   3   3   1   1   1   1]\n",
      " [  0   6  26  15  26  26   1   1   1   1   1   1   1]\n",
      " [  0 139 114 129 122 139  27   8   1   1   1   1   1]]\n",
      "\n",
      "back to words\n",
      "['משתמש:צלף/!', 'סימן קריאה', 'תבנית:!!', '$9.99', \"תבנית:'\"]\n"
     ]
    }
   ],
   "source": [
    "# Here's how you cast lines into ids and backwards.\n",
    "batch_lines = all_words[:5]\n",
    "batch_ids = inp_voc.to_matrix(batch_lines)\n",
    "batch_lines_restored = inp_voc.to_lines(batch_ids)\n",
    "\n",
    "print(\"lines\")\n",
    "print(batch_lines)\n",
    "print(\"\\nwords to ids (0 = bos, 1 = eos):\")\n",
    "print(batch_ids)\n",
    "print(\"\\nback to words\")\n",
    "print(batch_lines_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw word/translation length distributions to estimate the scope of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   16.,    67.,  1893.,  4238.,  6268.,  8015.,  8186.,  7720.,\n",
       "         7837.,  8957.,  9696., 10609., 10733., 10246.,  9015.,  7622.,\n",
       "         6313.,  5386.,  4372.,  3587.]),\n",
       " array([ 1.  ,  1.95,  2.9 ,  3.85,  4.8 ,  5.75,  6.7 ,  7.65,  8.6 ,\n",
       "         9.55, 10.5 , 11.45, 12.4 , 13.35, 14.3 , 15.25, 16.2 , 17.15,\n",
       "        18.1 , 19.05, 20.  ]),\n",
       " <a list of 20 Patch objects>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=[8, 4])\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"words\")\n",
    "plt.hist(list(map(len, all_words)), bins=20)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('translations')\n",
    "plt.hist(list(map(len, all_translations)), bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: deploy encoder-decoder (1 point)\n",
    "\n",
    "__assignment starts here__\n",
    "\n",
    "Our architecture consists of two main blocks:\n",
    "* Encoder reads words character by character and outputs code vector (usually a function of last RNN state)\n",
    "* Decoder takes that code vector and produces translations character by character\n",
    "\n",
    "Than it gets fed into a model that follows this simple interface:\n",
    "* __`model(inp, out, **flags) -> logp`__ - takes symbolic int32 matrices of hebrew words and their english translations. Computes the log-probabilities of all possible english characters given english prefices and hebrew word.\n",
    "* __`model.translate(inp, **flags) -> out, logp`__ - takes symbolic int32 matrix of hebrew words, produces output tokens sampled from the model and output log-probabilities for all possible tokens at each tick.\n",
    "  * if given flag __`greedy=True`__, takes most likely next token at each iteration. Otherwise samples with next token probabilities predicted by model.\n",
    "\n",
    "That's all! It's as hard as it gets. With those two methods alone you can implement all kinds of prediction and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basic_model_torch import BasicTranslationModel\n",
    "model = BasicTranslationModel(inp_voc, out_voc,\n",
    "                              emb_size=64, hid_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_voc.eos_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample translations:\n",
      " tensor([[  0, 171,  88, 181, 196, 128, 195, 218,  37,  28, 127,  57,  74,  17,\n",
      "          91, 245, 114, 131, 100, 189,  66, 274, 145, 160, 252,  31, 110,   7,\n",
      "          30,  30, 180,  39, 279, 208, 192, 216,  61, 235,  41, 113, 229, 235,\n",
      "         277, 147, 128,  15, 207, 116, 265,  24, 219, 200, 145,   5,  26, 224,\n",
      "          25, 131, 196,  58, 213, 205,  74, 242, 252,  30,  91, 258,  76,  97,\n",
      "         186, 166, 180,  11,  84,  88,  16, 245, 276, 174, 169,   4, 205,  33,\n",
      "          51, 228, 278, 179, 148,  63,  60, 144,  35, 147, 213, 125, 216, 254,\n",
      "          36, 179],\n",
      "        [  0,   9, 140,  50, 206, 115, 118, 216,  97,   8, 209, 100,  31, 146,\n",
      "          15, 281, 109, 135, 244, 207, 139,  81,  56, 216,  16,  77, 227, 225,\n",
      "          47,  15, 262,   5,  30, 145, 110,  92,  87, 145, 130, 234,  68, 178,\n",
      "          59, 257, 206, 104, 116,  37, 266, 176, 245, 268,  72, 122, 184, 133,\n",
      "          96, 201, 120,  11, 236,  39, 111, 279, 227,   9,  12, 219, 190,   4,\n",
      "         275,  19,  65,  87, 208,  15,  52, 195, 191, 265,  29, 241,  84, 116,\n",
      "         181, 183, 210, 133, 130, 199,  98, 143, 136, 276, 198,  21, 179, 216,\n",
      "         260, 143],\n",
      "        [  0, 101, 186, 175, 251, 114, 210, 204,  78, 148,  94, 195,  54, 133,\n",
      "         280, 223, 189,  98, 114, 256,  18, 256,  45,  10, 109, 214, 127, 152,\n",
      "          35,  94, 177, 204,  26, 177, 222, 237, 219, 195,  11,  44, 253, 172,\n",
      "         142, 171,  60, 148, 223, 278, 141, 129, 136,   0,  91,  10,   6,  30,\n",
      "          36,  36, 130, 177,  14, 213,  15,  40,  82, 182,  30,  91, 107, 122,\n",
      "          39, 159, 224,  69, 192,  42,  77,  28, 128,  16, 206,  33,   6, 103,\n",
      "         104,  33, 130,  93, 229, 148, 185, 187,  81, 203, 266,  93, 273, 159,\n",
      "         130, 260]])\n",
      "Log-probabilities at each step:\n",
      " tensor([[[  0.0000, -69.0776, -69.0776,  ..., -69.0776, -69.0776, -69.0776],\n",
      "         [ -5.7390,  -5.7479,  -5.6267,  ...,  -5.5586,  -5.7354,  -5.6236],\n",
      "         [ -5.6895,  -5.7020,  -5.6266,  ...,  -5.5280,  -5.6638,  -5.6874],\n",
      "         ...,\n",
      "         [ -5.7443,  -5.5768,  -5.5395,  ...,  -5.7562,  -5.6430,  -5.7512],\n",
      "         [ -5.7061,  -5.6099,  -5.5943,  ...,  -5.6967,  -5.6923,  -5.6757],\n",
      "         [ -5.6102,  -5.5986,  -5.5582,  ...,  -5.7862,  -5.5141,  -5.7896]],\n",
      "\n",
      "        [[  0.0000, -69.0776, -69.0776,  ..., -69.0776, -69.0776, -69.0776],\n",
      "         [ -5.7283,  -5.7610,  -5.5535,  ...,  -5.5326,  -5.7250,  -5.6187],\n",
      "         [ -5.6867,  -5.5427,  -5.5319,  ...,  -5.5535,  -5.6238,  -5.6357],\n",
      "         ...,\n",
      "         [ -5.5209,  -5.5806,  -5.6635,  ...,  -5.4787,  -5.6655,  -5.7843],\n",
      "         [ -5.6334,  -5.5009,  -5.6148,  ...,  -5.5764,  -5.7330,  -5.8146],\n",
      "         [ -5.6273,  -5.6450,  -5.6319,  ...,  -5.7345,  -5.7124,  -5.6782]],\n",
      "\n",
      "        [[  0.0000, -69.0776, -69.0776,  ..., -69.0776, -69.0776, -69.0776],\n",
      "         [ -5.7803,  -5.6898,  -5.6323,  ...,  -5.5385,  -5.7012,  -5.6584],\n",
      "         [ -5.7673,  -5.7012,  -5.6242,  ...,  -5.5823,  -5.6120,  -5.5648],\n",
      "         ...,\n",
      "         [ -5.7602,  -5.8412,  -5.7274,  ...,  -5.8625,  -5.6644,  -5.4274],\n",
      "         [ -5.6662,  -5.7233,  -5.7956,  ...,  -5.6742,  -5.7245,  -5.4986],\n",
      "         [ -5.6651,  -5.6044,  -5.7416,  ...,  -5.7189,  -5.6983,  -5.5732]]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Play around with symbolic_translate and symbolic_score\n",
    "inp = torch.tensor(np.random.randint(0, 10, [3, 5]), dtype=torch.int64)\n",
    "out = torch.tensor(np.random.randint(0, 10, [3, 5]), dtype=torch.int64)\n",
    "\n",
    "# translate inp (with untrained model)\n",
    "sampled_out, logp = model.translate(inp, greedy=False)\n",
    "\n",
    "print(\"Sample translations:\\n\", sampled_out)\n",
    "print(\"Log-probabilities at each step:\\n\", logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1],\n",
       "         [3],\n",
       "         [6],\n",
       "         [2],\n",
       "         [8]],\n",
       "\n",
       "        [[5],\n",
       "         [7],\n",
       "         [8],\n",
       "         [6],\n",
       "         [1]],\n",
       "\n",
       "        [[4],\n",
       "         [3],\n",
       "         [4],\n",
       "         [7],\n",
       "         [4]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[:, :, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.gather(logp, dim=2, index=out[:, :, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbolic_score output:\n",
      " tensor([[[  0.0000, -69.0776, -69.0776,  ..., -69.0776, -69.0776, -69.0776],\n",
      "         [ -5.6068,  -5.5597,  -5.6679,  ...,  -5.5788,  -5.6419,  -5.5218],\n",
      "         [ -5.6219,  -5.6089,  -5.5768,  ...,  -5.6279,  -5.5540,  -5.6284],\n",
      "         [ -5.6322,  -5.6214,  -5.5124,  ...,  -5.6111,  -5.5172,  -5.6222],\n",
      "         [ -5.6977,  -5.6435,  -5.7145,  ...,  -5.5535,  -5.5786,  -5.6635]],\n",
      "\n",
      "        [[  0.0000, -69.0776, -69.0776,  ..., -69.0776, -69.0776, -69.0776],\n",
      "         [ -5.6861,  -5.7054,  -5.8092,  ...,  -5.6029,  -5.6106,  -5.7162],\n",
      "         [ -5.6559,  -5.6954,  -5.6637,  ...,  -5.5665,  -5.5755,  -5.6863],\n",
      "         [ -5.7463,  -5.8196,  -5.7986,  ...,  -5.5661,  -5.7185,  -5.7118],\n",
      "         [ -5.6110,  -5.6669,  -5.8736,  ...,  -5.5892,  -5.7801,  -5.6469]],\n",
      "\n",
      "        [[  0.0000, -69.0776, -69.0776,  ..., -69.0776, -69.0776, -69.0776],\n",
      "         [ -5.6765,  -5.6777,  -5.7922,  ...,  -5.5350,  -5.6467,  -5.6675],\n",
      "         [ -5.7123,  -5.6914,  -5.8680,  ...,  -5.5272,  -5.6358,  -5.7181],\n",
      "         [ -5.7564,  -5.8221,  -5.8943,  ...,  -5.5835,  -5.7301,  -5.7481],\n",
      "         [ -5.6772,  -5.7448,  -5.6796,  ...,  -5.6468,  -5.6049,  -5.7738]]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Log-probabilities of output tokens:\n",
      " tensor([[[-69.0776],\n",
      "         [ -5.6163],\n",
      "         [ -5.6219],\n",
      "         [ -5.7170],\n",
      "         [ -5.6385]],\n",
      "\n",
      "        [[-69.0776],\n",
      "         [ -5.6861],\n",
      "         [ -5.6406],\n",
      "         [ -5.7497],\n",
      "         [ -5.8736]],\n",
      "\n",
      "        [[-69.0776],\n",
      "         [ -5.6721],\n",
      "         [ -5.5495],\n",
      "         [ -5.8383],\n",
      "         [ -5.6796]]], grad_fn=<GatherBackward>)\n"
     ]
    }
   ],
   "source": [
    "# score logp(out | inp) with untrained input\n",
    "logp = model(inp, out)\n",
    "print(\"Symbolic_score output:\\n\", logp)\n",
    "\n",
    "print(\"Log-probabilities of output tokens:\\n\",\n",
    "      torch.gather(logp, dim=2, index=out[:, :, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(lines, max_len=MAX_OUTPUT_LENGTH):\n",
    "    \"\"\"\n",
    "    You are given a list of input lines. \n",
    "    Make your neural network translate them.\n",
    "    :return: a list of output lines\n",
    "    \"\"\"\n",
    "    # Convert lines to a matrix of indices\n",
    "    lines_ix = inp_voc.to_matrix(lines)\n",
    "    lines_ix = torch.tensor(lines_ix, dtype=torch.int64)\n",
    "\n",
    "    # Compute translations in form of indices\n",
    "    trans_ix = model.translate(lines_ix, greedy=True, max_len=max_len)[0]\n",
    "\n",
    "    # Convert translations back into strings\n",
    "    return out_voc.to_lines(trans_ix.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample inputs: ['משתמש:צלף/!' 'סימן קריאה' 'תבנית:!!']\n",
      "Dummy translations: ['aπk→уųž白אó²5בصюæфųž', 'aπk→уųž白אó²5בصюæфųž', 'a白üπk→уųž白אó²5בصюæф']\n",
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample inputs:\", all_words[:3])\n",
    "print(\"Dummy translations:\", translate(all_words[:3]))\n",
    "trans = translate(all_words[:3])\n",
    "\n",
    "assert translate(all_words[:3]) == translate(\n",
    "    all_words[:3]), \"make sure translation is deterministic (use greedy=True and disable any noise layers)\"\n",
    "assert type(translate(all_words[:3])) is list and (type(translate(all_words[:1])[0]) is str or type(\n",
    "    translate(all_words[:1])[0]) is unicode), \"translate(lines) must return a sequence of strings!\"\n",
    "# note: if translation freezes, make sure you used max_len parameter\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring function\n",
    "\n",
    "LogLikelihood is a poor estimator of model performance.\n",
    "* If we predict zero probability once, it shouldn't ruin entire model.\n",
    "* It is enough to learn just one translation if there are several correct ones.\n",
    "* What matters is how many mistakes model's gonna make when it translates!\n",
    "\n",
    "Therefore, we will use minimal Levenshtein distance. It measures how many characters do we need to add/remove/replace from model translation to make it perfect. Alternatively, one could use character-level BLEU/RougeL or other similar metrics.\n",
    "\n",
    "The catch here is that Levenshtein distance is not differentiable: it isn't even continuous. We can't train our neural network to maximize it by gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance  # !pip install editdistance\n",
    "\n",
    "\n",
    "def get_distance(word, trans):\n",
    "    \"\"\"\n",
    "    A function that takes word and predicted translation\n",
    "    and evaluates (Levenshtein's) edit distance to closest correct translation\n",
    "    \"\"\"\n",
    "    references = word_to_translation[word]\n",
    "    assert len(references) != 0, \"wrong/unknown word\"\n",
    "    return min(editdistance.eval(trans, ref) for ref in references)\n",
    "\n",
    "\n",
    "def score(words, bsize=100):\n",
    "    \"\"\"a function that computes levenshtein distance for bsize random samples\"\"\"\n",
    "    assert isinstance(words, np.ndarray)\n",
    "\n",
    "    batch_words = np.random.choice(words, size=bsize, replace=False)\n",
    "    batch_trans = translate(batch_words)\n",
    "\n",
    "    distances = list(map(get_distance, batch_words, batch_trans))\n",
    "\n",
    "    return np.array(distances, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18.6, 18.6, 19.0, 18.5, 18.8]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should be around 5-50 and decrease rapidly after training :)\n",
    "[score(test_words, 10).mean() for _ in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Supervised pre-training (2 points)\n",
    "\n",
    "Here we define a function that trains our model through maximizing log-likelihood a.k.a. minimizing crossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def sample_batch(words, word_to_translation, batch_size):\n",
    "    \"\"\"\n",
    "    sample random batch of words and random correct translation for each word\n",
    "    example usage:\n",
    "    batch_x,batch_y = sample_batch(train_words, word_to_translations,10)\n",
    "    \"\"\"\n",
    "    # choose words\n",
    "    batch_words = np.random.choice(words, size=batch_size)\n",
    "\n",
    "    # choose translations\n",
    "    batch_trans_candidates = list(map(word_to_translation.get, batch_words))\n",
    "    batch_trans = list(map(random.choice, batch_trans_candidates))\n",
    "#     return batch_words, batch_trans\n",
    "    return inp_voc.to_matrix(batch_words), out_voc.to_matrix(batch_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:\n",
      "[[  0 130 118 133 122 113 128   2 130 121 122 114 129 130   1]\n",
      " [  0 130 118 133 118 135   8 113 129 122   1   1   1   1   1]\n",
      " [  0 130 118 113 127 118   2 121 118 122 118 116 117   1   1]]\n",
      "Target:\n",
      "[[  0  51  53  38  42  33  46   2  51  52  37  54  37  46  51   1]\n",
      " [  0  51  47  48  47 102  33  46  41   1   1   1   1   1   1   1]\n",
      " [  0  51  47  37  45  53   2  52  47  57  47  36  33   1   1   1]]\n"
     ]
    }
   ],
   "source": [
    "bx, by = sample_batch(train_words, word_to_translation, batch_size=3)\n",
    "print(\"Source:\")\n",
    "print(bx)\n",
    "print(\"Target:\")\n",
    "print(by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basic_model_torch import infer_length, infer_mask, to_one_hot\n",
    "\n",
    "\n",
    "def compute_loss_on_batch(input_sequence, reference_answers):\n",
    "    \"\"\" Compute crossentropy loss given a batch of sources and translations \"\"\"\n",
    "    input_sequence = torch.tensor(inp_voc.to_matrix(input_sequence), dtype=torch.int64)\n",
    "    reference_answers = torch.tensor(out_voc.to_matrix(reference_answers), dtype=torch.int64)\n",
    "\n",
    "    # Compute log-probabilities of all possible tokens at each step. Use model interface.\n",
    "#     print('init shape: ', reference_answers.shape)\n",
    "    answer, logprobs_seq = model.translate(input_sequence, greedy=False, max_len=reference_answers.shape[1])\n",
    "    if logprobs_seq.shape[1] != reference_answers.shape[1]:\n",
    "        print('suka')\n",
    "        return answer, logprobs_seq\n",
    "\n",
    "    # compute elementwise crossentropy as negative log-probabilities of reference_answers.\n",
    "    crossentropy = - \\\n",
    "        torch.sum(logprobs_seq *\n",
    "                  to_one_hot(reference_answers, len(out_voc)), dim=-1)\n",
    "    assert crossentropy.dim(\n",
    "    ) == 2, \"please return elementwise crossentropy, don't compute mean just yet\"\n",
    "\n",
    "    # average with mask\n",
    "    mask = infer_mask(reference_answers, out_voc.eos_ix)\n",
    "    loss = torch.sum(crossentropy * mask) / torch.sum(mask)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  tensor(5.3519, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test it\n",
    "loss = compute_loss_on_batch(*sample_batch(train_words, word_to_translation, 3))\n",
    "print('loss = ', loss)\n",
    "\n",
    "assert loss.item() > 0.0\n",
    "loss.backward()\n",
    "for w in model.parameters():\n",
    "    assert w.grad is not None and torch.max(torch.abs(w.grad)).item() != 0, \\\n",
    "        \"Loss is not differentiable w.r.t. a weight with shape %s. Check comput_loss_on_batch.\" % (\n",
    "            w.size(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Actually train the model\n",
    "\n",
    "Minibatches and stuff..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm, trange  # or use tqdm_notebook,tnrange\n",
    "\n",
    "loss_history = []\n",
    "editdist_history = []\n",
    "entropy_history = []\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 19, 283]), torch.Size([32, 19]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_sequence, reference_answers = sample_batch(train_words, word_to_translation, 32)\n",
    "# input_sequence = torch.tensor(inp_voc.to_matrix(input_sequence), dtype=torch.int64)\n",
    "# reference_answers = torch.tensor(out_voc.to_matrix(reference_answers), dtype=torch.int64)\n",
    "answer = model.translate(input_sequence, greedy=False, max_len=reference_answers.shape[1])[1]\n",
    "\n",
    "answer.shape, reference_answers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([ len(el) for el in out_voc.to_lines(loss[0].data.numpy())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 19]), torch.Size([32, 19, 283]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss[0].shape, loss[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 22]), torch.Size([32, 22]))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequence = torch.tensor(inp_voc.to_matrix(input_sequence), dtype=torch.int64)\n",
    "reference_answers = torch.tensor(out_voc.to_matrix(reference_answers), dtype=torch.int64)\n",
    "input_sequence.shape, reference_answers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 18]), torch.Size([32, 18, 283]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer, logprobs_seq = model.translate(input_sequence, greedy=False, max_len=reference_answers.shape[1])\n",
    "answer.shape, logprobs_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 21]), torch.Size([32, 21, 283]))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss[0].shape, loss[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basic_model_torch import BasicTranslationModel\n",
    "model = BasicTranslationModel(inp_voc, out_voc,\n",
    "                              emb_size=283, hid_size=512)\n",
    "\n",
    "loss_history = []\n",
    "editdist_history = []\n",
    "entropy_history = []\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "decayRate = 0.999\n",
    "my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=opt, gamma=decayRate)\n",
    "grads = [[] for _ in range(len(list(model.parameters())))]\n",
    "lrs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "decayRate = 0.999\n",
    "my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=opt, gamma=decayRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicTranslationModel(\n",
      "  (emb_inp): Embedding(189, 64)\n",
      "  (emb_out): Embedding(283, 64)\n",
      "  (enc0): GRU(64, 256, batch_first=True)\n",
      "  (dec_start): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (dec0): GRUCell(64, 256)\n",
      "  (logits): Linear(in_features=256, out_features=283, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-174-7f560d3b9498>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.7417\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m11.2250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m     \"\"\"\n\u001b[0;32m--> 668\u001b[0;31m     \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;31m# catch default case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "torch.norm(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "for j, w in enumerate(model.parameters()):\n",
    "    print(j)\n",
    "    grads[j].append(float(torch.norm(w.grad).data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.514474781081598e-05]\n"
     ]
    }
   ],
   "source": [
    "print(my_lr_scheduler.get_lr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.0380529, dtype=float32)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(w.grad).data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words_sorted = sorted(train_words, key=lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117102"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_words_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0, 125, 113, 118, 130,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1],\n",
       "       [  0, 136, 122, 130, 137,   2, 113, 139, 122, 118, 133, 122, 117,\n",
       "          2, 122, 118, 129, 130,   1],\n",
       "       [  0, 136, 118, 125, 121, 118, 137,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1],\n",
       "       [  0, 129, 113, 118, 127, 128,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1],\n",
       "       [  0, 137, 114, 122, 128,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1],\n",
       "       [  0, 115, 118, 137, 122, 137, 117,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1],\n",
       "       [  0, 116, 122, 118, 136,   2, 129, 122, 118, 136, 126,   2, 125,\n",
       "        129, 135, 120,   1,   1,   1],\n",
       "       [  0,  51,  32,  34,  32,  34,  50,  12,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1],\n",
       "       [  0, 138, 118, 127, 137,   2, 127, 130, 123,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1],\n",
       "       [  0, 133, 118, 129, 136, 135, 122, 122, 139,   2, 137, 122, 127,\n",
       "        128,   1,   1,   1,   1,   1],\n",
       "       [  0, 130, 114, 122, 125,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1],\n",
       "       [  0, 139, 137, 114, 118, 139,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1],\n",
       "       [  0, 117, 113, 118, 116, 117,   2, 125, 138, 127, 120, 117,   1,\n",
       "          1,   1,   1,   1,   1,   1],\n",
       "       [  0, 122, 137, 116, 129, 117,   2, 113, 125, 118, 121, 122, 128,\n",
       "          1,   1,   1,   1,   1,   1],\n",
       "       [  0, 116, 118, 129, 125, 116,   2, 118, 122, 129, 122, 136, 118,\n",
       "        121,   1,   1,   1,   1,   1],\n",
       "       [  0, 136, 137, 125,   2, 138, 118, 118, 137, 135, 138, 122, 125,\n",
       "        116,   1,   1,   1,   1,   1],\n",
       "       [  0, 121, 118, 127, 122,   2, 125, 122,   2, 115,   8, 118, 129,\n",
       "        130,   1,   1,   1,   1,   1],\n",
       "       [  0, 120, 114, 122, 122, 137,   2, 130, 114, 122, 118, 125, 117,\n",
       "          1,   1,   1,   1,   1,   1],\n",
       "       [  0, 122, 137, 118, 128,   2, 114, 137, 118, 136,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1],\n",
       "       [  0, 113, 122, 125, 122, 136, 122, 121,   2, 127, 122, 129, 118,\n",
       "        137,   1,   1,   1,   1,   1],\n",
       "       [  0, 136, 118, 118, 122, 128,   2, 114, 137, 127, 122, 122, 130,\n",
       "        121, 137,   1,   1,   1,   1],\n",
       "       [  0,  54,  32,  45,  35,  36,  49,  40,  45,  38,   2,  50,  51,\n",
       "         32,  49,  50,   1,   1,   1],\n",
       "       [  0, 116, 118, 116,   2, 116, 129, 118, 128,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1],\n",
       "       [  0, 122, 117, 125,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1],\n",
       "       [  0, 127, 113, 139, 122, 122, 130, 128,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1],\n",
       "       [  0, 117, 127, 129, 118, 128,   2, 117, 127, 125, 116, 122, 114,\n",
       "        122, 122, 126,   1,   1,   1],\n",
       "       [  0, 127, 139, 119,   2, 127, 122, 126,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1],\n",
       "       [  0, 115,   8, 122, 122,  15,   2, 130, 122,  15,   2, 135,   8,\n",
       "        129, 116, 118, 137,   1,   1],\n",
       "       [  0, 113, 137, 124, 132,   2, 113, 133, 118, 137,   2, 133, 129,\n",
       "        122, 126,   1,   1,   1,   1],\n",
       "       [  0, 127, 122, 136, 137, 118,  14, 127, 120, 138, 114,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1],\n",
       "       [  0, 127, 119, 137, 131,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1],\n",
       "       [  0, 129, 113, 130,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1]], dtype=int32)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 44, 33, 47, 51,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1],\n",
       "       [ 0, 57, 47, 46, 33, 51,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1],\n",
       "       [ 0, 43, 47, 44, 52, 53, 50,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1],\n",
       "       [ 0, 46, 33, 53, 45, 33, 46,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1],\n",
       "       [ 0, 50, 33, 34, 41, 46,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1],\n",
       "       [ 0, 39, 53, 50, 41, 50, 33,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1],\n",
       "       [ 0, 36, 53, 43, 37,  2, 46, 53, 43, 37, 45,  2, 38, 47, 50, 37,\n",
       "        54, 37, 50,  1],\n",
       "       [ 0, 52, 33, 35, 33, 35, 51, 12,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1],\n",
       "       [ 0, 51, 35, 50, 37, 37, 46, 51, 33, 54, 37, 50,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1],\n",
       "       [ 0, 52, 40, 47, 45, 33, 37,  8, 51,  2, 38, 53, 46, 35, 52, 41,\n",
       "        47, 46,  1,  1],\n",
       "       [ 0, 51, 37, 34, 41, 44,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1],\n",
       "       [ 0, 52, 33, 50, 34, 53, 52,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1],\n",
       "       [ 0, 47, 36, 37,  2, 52, 47,  2, 42, 47, 57,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1],\n",
       "       [ 0, 57, 33, 50, 36, 37, 46, 33,  2, 33, 44, 47, 52, 41, 46,  1,\n",
       "         1,  1,  1,  1],\n",
       "       [ 0, 36, 47, 46, 33, 44, 36,  2, 55, 41, 46, 46, 41, 35, 47, 52,\n",
       "        52,  1,  1,  1],\n",
       "       [ 0, 43, 33, 50, 44,  2, 51, 35, 40, 55, 33, 50, 58, 51, 35, 40,\n",
       "        41, 44, 36,  1],\n",
       "       [ 0, 52, 47, 45, 45, 57,  2, 44, 37, 37,  2, 42, 47, 46, 37, 51,\n",
       "         1,  1,  1,  1],\n",
       "       [ 0, 42, 33, 54, 41, 37, 50,  2, 51, 33, 54, 41, 47, 44, 33,  1,\n",
       "         1,  1,  1,  1],\n",
       "       [ 0, 57, 33, 50, 47, 46,  2, 34, 50, 47, 47, 43,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1],\n",
       "       [ 0, 41, 44, 44, 41, 35, 41, 52,  2, 45, 41, 46, 47, 50,  1,  1,\n",
       "         1,  1,  1,  1],\n",
       "       [ 0, 43, 37, 54, 41, 46,  2, 34, 37, 50, 45, 37, 41, 51, 52, 37,\n",
       "        50,  1,  1,  1],\n",
       "       [ 0, 55, 33, 46, 36, 37, 50, 41, 46, 39,  2, 51, 52, 33, 50, 51,\n",
       "         1,  1,  1,  1],\n",
       "       [ 0, 36, 33, 54, 41, 36,  2, 36, 33, 46, 47, 46,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1],\n",
       "       [ 0, 57, 33, 40, 37, 44,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1],\n",
       "       [ 0, 45, 33, 52, 40, 41, 42, 51, 37, 46,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1],\n",
       "       [ 0, 39, 33, 53, 45, 41, 41,  2, 51, 33, 44, 33, 33, 45,  1,  1,\n",
       "         1,  1,  1,  1],\n",
       "       [ 0, 38, 41, 50, 37,  2, 51, 48, 50, 41, 46, 43, 44, 37, 50,  1,\n",
       "         1,  1,  1,  1],\n",
       "       [ 0, 42, 15,  2, 35, 40, 33, 46, 36, 47, 50,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1],\n",
       "       [ 0, 39, 50, 37, 57, 14, 38, 33, 35, 37, 36,  2, 51, 37, 46, 39,\n",
       "        41,  1,  1,  1],\n",
       "       [ 0, 45, 41, 35, 50, 47, 35, 47, 45, 48, 53, 52, 37, 50,  1,  1,\n",
       "         1,  1,  1,  1],\n",
       "       [ 0, 45, 41, 58, 50, 33,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1],\n",
       "       [ 0, 46, 33, 51,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1]], dtype=int32)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3659"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_words_sorted) // 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/25000 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-237-d558e6aa8e05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# train with backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference_answers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-128-ea77b50f5971>\u001b[0m in \u001b[0;36mcompute_loss_on_batch\u001b[0;34m(input_sequence, reference_answers)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference_answers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"\"\" Compute crossentropy loss given a batch of sources and translations \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0minput_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_voc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mreference_answers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_voc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreference_answers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/Practical_RL/week07_seq2seq/voc.py\u001b[0m in \u001b[0;36mto_matrix\u001b[0;34m(self, lines, max_len)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mrow_ix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_to_ix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow_ix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "\n",
    "# for epoch in range(3):\n",
    "#     for i in trange(0, len(train_words_sorted), 32):\n",
    "for i in trange(25000):\n",
    "    input_sequence, reference_answers = sample_batch(train_words, word_to_translation, 32)\n",
    "#         batch_words = train_words_sorted[i:i+32]\n",
    "\n",
    "#         # choose translations\n",
    "#         batch_trans_candidates = list(map(word_to_translation.get, batch_words))\n",
    "#         batch_trans = list(map(random.choice, batch_trans_candidates))\n",
    "#         input_sequence, reference_answers = batch_words, batch_trans\n",
    "#         return batch_words, batch_trans\n",
    "\n",
    "    # train with backprop\n",
    "    opt.zero_grad()\n",
    "    loss = compute_loss_on_batch(input_sequence, reference_answers)\n",
    "    if type(loss) is tuple:\n",
    "        break\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    lrs.append(my_lr_scheduler.get_lr()[0])\n",
    "#         my_lr_scheduler.step()\n",
    "\n",
    "\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "    if (i) % (32 * REPORT_FREQ) == 0:\n",
    "        for j, w in enumerate(model.parameters()):\n",
    "            grads[j].append(torch.norm(w.grad).data.numpy())\n",
    "#     if (i+1) % 10 == 0:\n",
    "\n",
    "        clear_output(True)\n",
    "        current_scores = score(test_words)\n",
    "        editdist_history.append(current_scores.mean())\n",
    "        print(\"epoch=%d, llh=%.3f, mean score=%.3f\" %\n",
    "              (epoch + 1, np.mean(loss_history[-10:]), np.mean(editdist_history[-10:])))\n",
    "        plt.figure(figsize=(16, 16))\n",
    "        plt.subplot(541)\n",
    "        plt.title('train loss / traning time')\n",
    "        plt.plot(loss_history)\n",
    "        plt.grid()\n",
    "        plt.subplot(542)\n",
    "        plt.title('val score distribution')\n",
    "        plt.hist(current_scores, bins=20)\n",
    "        plt.subplot(543)\n",
    "        plt.title('val score / traning time (lower is better)')\n",
    "        plt.plot(editdist_history)\n",
    "        plt.subplot(544)\n",
    "        plt.title('lr')\n",
    "        plt.plot(lrs)\n",
    "        for j, grad in enumerate(grads):\n",
    "            index = j + 5\n",
    "#             ax2 = plt.add_subplot(5, 4, j)\n",
    "            plt.subplot(5, 4, index)\n",
    "            plt.title('grad %d' % j)\n",
    "            plt.plot(grad)\n",
    "\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.1503e-04,  3.3035e-04,  8.3798e-05,  ..., -1.0255e-05,\n",
      "          1.3919e-04, -4.1442e-05],\n",
      "        [ 4.2403e-04, -3.6716e-04,  3.5927e-04,  ..., -4.2316e-04,\n",
      "         -4.9905e-04,  5.2850e-04],\n",
      "        [-2.0898e-04, -1.6004e-04, -1.4465e-06,  ..., -2.4114e-05,\n",
      "         -4.4442e-06, -6.1975e-05],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]])\n",
      "tensor([[ 7.3748e-05, -2.5243e-05,  1.4648e-05,  ..., -1.3894e-05,\n",
      "          3.3051e-05, -8.2736e-05],\n",
      "        [-2.7911e-05,  3.5900e-05,  2.3998e-06,  ...,  7.2839e-06,\n",
      "          6.4600e-07,  2.3654e-05],\n",
      "        [-2.3844e-05, -1.5580e-04,  4.1936e-05,  ...,  1.2522e-04,\n",
      "         -2.7927e-05,  2.8210e-05],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]])\n",
      "tensor([[ 5.2651e-05, -1.0386e-04,  1.0024e-05,  ...,  2.1088e-05,\n",
      "         -9.6252e-05,  1.3550e-04],\n",
      "        [ 7.6387e-05, -1.2082e-05, -1.3464e-05,  ..., -2.3631e-05,\n",
      "         -4.6686e-05,  1.1966e-04],\n",
      "        [ 2.0214e-05, -7.6585e-05,  1.2749e-05,  ...,  4.1034e-05,\n",
      "         -3.5207e-05,  1.1449e-04],\n",
      "        ...,\n",
      "        [-3.4334e-04, -2.0412e-04, -4.2585e-05,  ...,  4.5349e-04,\n",
      "         -5.1681e-05,  7.9791e-04],\n",
      "        [ 9.6357e-04, -5.2503e-04,  1.5025e-03,  ..., -1.0344e-03,\n",
      "         -2.1784e-03, -1.0382e-03],\n",
      "        [-3.2426e-04, -2.3057e-04, -2.2317e-04,  ...,  5.0918e-04,\n",
      "          4.0133e-05, -8.6456e-05]])\n",
      "tensor([[ 9.4855e-06, -1.0679e-05,  1.2314e-05,  ..., -1.3165e-05,\n",
      "         -3.3055e-05,  1.8510e-05],\n",
      "        [ 9.0339e-07,  6.3631e-06,  1.2369e-06,  ..., -1.3580e-05,\n",
      "          9.5185e-07,  3.7310e-06],\n",
      "        [-4.3136e-06, -1.0116e-05,  3.8946e-06,  ...,  2.3399e-06,\n",
      "         -2.9223e-05, -6.5360e-06],\n",
      "        ...,\n",
      "        [-1.0898e-04, -1.8499e-04,  7.4080e-05,  ...,  8.5008e-05,\n",
      "         -2.9977e-04, -1.3680e-04],\n",
      "        [ 4.7625e-04,  3.4492e-04, -1.5390e-06,  ..., -4.7414e-04,\n",
      "          5.1623e-04,  5.5062e-04],\n",
      "        [-3.7046e-05, -6.0306e-05, -3.6547e-05,  ...,  5.9135e-05,\n",
      "         -9.6580e-05, -5.1661e-05]])\n",
      "tensor([-4.4940e-05,  1.0569e-05, -8.4355e-05,  9.9476e-05, -8.7722e-05,\n",
      "        -2.8200e-05, -3.2297e-05, -9.3338e-05, -5.5055e-05,  1.5031e-04,\n",
      "        -7.7377e-07,  3.1142e-05,  2.1239e-05, -4.6479e-05,  1.1683e-06,\n",
      "        -3.8748e-05,  3.3702e-05,  5.8185e-05, -1.4751e-04, -8.7497e-06,\n",
      "        -5.5864e-06, -2.0057e-05, -1.2148e-05, -5.8650e-05, -2.9978e-05,\n",
      "        -4.7708e-05, -2.5155e-06,  1.5070e-04,  1.6763e-04,  1.6638e-04,\n",
      "         7.7958e-05,  3.1045e-05, -6.2406e-05, -7.2028e-05, -1.0941e-05,\n",
      "        -1.9533e-05,  1.3366e-04,  4.7202e-06, -7.1393e-05, -1.2094e-05,\n",
      "        -8.5451e-05, -6.6477e-05,  8.1280e-05, -3.3549e-05, -9.0872e-05,\n",
      "         8.7019e-06, -1.7641e-05, -4.2387e-05,  1.9050e-05,  9.9676e-05,\n",
      "        -1.0925e-04, -5.0248e-05, -1.3581e-04, -3.1640e-05,  6.4015e-05,\n",
      "        -2.6906e-05, -5.9743e-05, -1.1454e-04,  4.5395e-06, -2.4989e-05,\n",
      "         1.1679e-05, -9.1390e-05,  9.9346e-05,  4.8448e-05,  8.4681e-05,\n",
      "        -2.8974e-05,  2.5201e-05,  8.1267e-05, -2.5705e-05, -7.9922e-05,\n",
      "         5.8160e-05,  8.6072e-05, -7.2100e-06,  8.3941e-05, -2.0054e-05,\n",
      "         2.8906e-04,  3.4538e-04,  6.4040e-05,  3.9240e-05,  1.2746e-04,\n",
      "         5.5391e-05, -1.2010e-04, -7.3884e-06,  1.1424e-06,  1.8147e-05,\n",
      "        -2.1857e-05, -8.3965e-05, -7.7176e-05,  4.1040e-05, -1.7513e-04,\n",
      "        -5.3501e-05, -8.5210e-05,  4.1892e-05, -1.2372e-04, -3.2549e-05,\n",
      "        -8.9011e-05,  1.7579e-04,  3.4402e-04,  1.2064e-04, -7.6907e-05,\n",
      "         1.2413e-05, -3.5999e-05,  2.4859e-05,  1.5614e-04, -5.8310e-05,\n",
      "         5.6666e-05,  4.8463e-05, -5.4153e-05,  3.3889e-05, -5.7075e-05,\n",
      "        -2.1648e-04,  4.4164e-04, -6.4053e-05, -1.0245e-04, -7.1220e-05,\n",
      "        -7.4460e-05, -7.8577e-05, -2.4789e-06, -3.5785e-05, -3.3610e-05,\n",
      "         5.7549e-05, -7.0507e-05,  2.8371e-04,  1.0646e-04, -9.9928e-05,\n",
      "         7.8183e-05, -5.0416e-05, -8.5705e-05, -1.5739e-06, -5.9043e-05,\n",
      "        -2.7875e-05,  8.0019e-05,  5.6807e-05,  3.2584e-05,  1.3149e-04,\n",
      "         1.2366e-04, -3.1577e-05,  3.0216e-05,  7.1107e-05, -1.5245e-04,\n",
      "        -1.0138e-04, -8.6229e-05,  3.8285e-05,  7.6283e-06, -1.8683e-04,\n",
      "         5.1542e-05, -1.4017e-05,  6.5534e-05,  4.4244e-04,  9.8300e-05,\n",
      "         2.1292e-04, -1.1639e-04, -5.2424e-05, -8.7740e-05, -5.3674e-05,\n",
      "        -8.0788e-05, -7.0420e-05, -7.2365e-05,  9.4402e-05,  8.1242e-05,\n",
      "         4.8772e-05, -1.1911e-04, -6.6592e-05,  2.1474e-05, -3.7583e-05,\n",
      "        -4.1990e-05,  1.3365e-04,  4.6033e-05,  8.6343e-06,  8.6049e-05,\n",
      "         2.7408e-05, -4.2770e-05, -6.0252e-05, -1.4753e-05, -7.7562e-05,\n",
      "        -5.3632e-05,  8.5964e-05, -9.2872e-05,  1.0870e-04, -6.2211e-05,\n",
      "        -1.1416e-04, -3.9830e-05,  4.1621e-05,  1.6119e-05, -7.4442e-05,\n",
      "        -1.8543e-04, -8.6417e-05,  1.6106e-04,  5.9413e-06, -1.2298e-04,\n",
      "         1.8619e-05, -1.4289e-05, -7.8100e-05, -3.2138e-05, -1.0161e-04,\n",
      "         1.9038e-05,  6.6903e-05, -6.1537e-06, -2.3655e-05,  4.6543e-05,\n",
      "        -1.1537e-04, -2.6131e-05,  1.8132e-04, -4.6350e-05, -1.1078e-04,\n",
      "        -1.1021e-04, -1.4451e-04,  4.7866e-05, -1.4357e-04,  2.1471e-05,\n",
      "        -6.9613e-05,  6.1141e-05,  2.1653e-05, -5.9651e-05, -1.3711e-04,\n",
      "        -1.8071e-05, -4.1531e-05, -1.1425e-04,  9.0630e-05, -4.0402e-05,\n",
      "         5.3727e-05, -9.6530e-05, -3.7798e-05,  3.5229e-05, -1.3669e-04,\n",
      "        -1.2998e-04, -7.2292e-06, -1.1867e-04, -1.4405e-04, -1.8524e-04,\n",
      "        -1.2303e-04, -4.7256e-05, -8.7901e-06, -6.4120e-05, -2.2298e-05,\n",
      "         5.1934e-06,  5.6567e-05, -7.8603e-05,  3.8643e-05, -5.2167e-05,\n",
      "        -7.4163e-06,  4.7353e-05,  3.5501e-05, -5.7432e-05,  4.1658e-05,\n",
      "        -4.4634e-05, -1.2131e-04, -3.9175e-05, -1.0734e-04, -6.0614e-05,\n",
      "         3.2601e-05,  2.3879e-05,  3.2551e-05,  2.5870e-05,  6.6743e-05,\n",
      "        -9.8267e-06,  4.4715e-05,  1.1872e-04,  2.6000e-05, -7.2720e-05,\n",
      "         7.2974e-05,  4.3819e-05,  1.6716e-05,  1.4046e-04,  5.5840e-05,\n",
      "         1.0844e-04,  5.0002e-05,  1.2843e-05,  7.1421e-05,  5.1037e-05,\n",
      "        -2.5778e-04, -3.2401e-06,  7.4658e-05, -7.3295e-05,  8.6040e-05,\n",
      "        -1.3580e-04,  2.9263e-05,  7.6404e-06,  9.7492e-05,  9.3271e-05,\n",
      "         1.0308e-04,  5.0875e-05,  3.1882e-05, -7.6681e-05, -1.0023e-04,\n",
      "        -1.5793e-04,  4.3922e-05,  8.2025e-05,  1.3690e-04,  5.0509e-05,\n",
      "        -1.3535e-05, -1.3696e-05,  2.1991e-04,  1.1951e-04, -7.6999e-05,\n",
      "         4.8214e-05,  4.4460e-05, -4.9307e-05, -1.1900e-04, -5.1497e-05,\n",
      "        -1.4958e-06,  3.9446e-05,  8.0249e-06,  3.6854e-05,  1.1494e-04,\n",
      "         3.6308e-05, -2.8711e-06, -1.5536e-04,  6.0571e-05,  1.0555e-04,\n",
      "        -7.1221e-06,  4.4669e-05,  1.4042e-04,  1.5112e-04,  3.7135e-05,\n",
      "        -1.7020e-05,  8.5303e-05, -4.1947e-04,  6.6736e-06, -1.8768e-04,\n",
      "        -3.2117e-05,  2.3105e-04,  1.8909e-05,  2.3329e-04,  1.7562e-04,\n",
      "         2.0531e-05, -7.2576e-05,  8.5020e-05,  7.6542e-05,  4.9019e-05,\n",
      "        -3.1451e-05, -1.3011e-04, -3.1968e-04,  1.0124e-04,  1.3559e-04,\n",
      "        -5.6200e-05,  7.8084e-05,  1.0889e-04,  6.2399e-05,  3.0473e-05,\n",
      "         5.2748e-05,  6.3056e-05,  9.7195e-05,  2.3807e-05, -1.2659e-04,\n",
      "         1.0433e-04, -2.0369e-06,  2.3481e-04,  2.3660e-04,  3.1905e-05,\n",
      "        -3.3476e-04,  1.3137e-04,  1.1924e-04, -2.1276e-04, -4.3678e-05,\n",
      "         9.4877e-06,  1.3116e-04,  5.3278e-05,  9.9840e-05, -4.2434e-05,\n",
      "        -2.1307e-04,  9.1635e-05,  4.9100e-05,  1.0683e-04,  6.7403e-05,\n",
      "         1.0043e-04,  1.6226e-04,  1.1329e-07, -4.5144e-05, -2.6108e-04,\n",
      "         1.0672e-04,  1.2217e-04, -2.9533e-05,  3.9106e-05,  8.7364e-05,\n",
      "         4.3370e-06, -1.0837e-04,  1.1840e-04, -3.0864e-04, -1.8145e-04,\n",
      "         1.1104e-04, -1.1204e-04,  3.5340e-05,  1.3869e-05,  3.9100e-06,\n",
      "         4.3889e-06, -3.0353e-04,  1.5362e-04,  1.8657e-04,  4.0386e-05,\n",
      "        -2.5485e-04,  1.1104e-04, -4.2176e-05, -9.2154e-05,  7.2000e-05,\n",
      "         4.2183e-05,  1.4432e-04,  1.1908e-04, -6.4179e-05,  1.1978e-04,\n",
      "         9.2654e-05,  5.3275e-05,  9.9549e-05, -3.4264e-05,  4.4536e-04,\n",
      "        -9.0217e-05, -1.0813e-04,  2.2225e-05,  1.3247e-04, -4.5963e-06,\n",
      "        -3.6101e-05,  8.9559e-05,  9.3487e-05,  5.0301e-05, -5.8123e-05,\n",
      "        -5.0962e-05,  1.1221e-04,  5.1149e-05,  8.0766e-05,  5.3672e-05,\n",
      "         4.2995e-05, -2.6533e-05, -2.7509e-04,  3.9394e-04,  2.5607e-05,\n",
      "         2.5997e-05, -3.5512e-05,  7.2381e-05,  3.4996e-05,  4.0932e-05,\n",
      "         2.7893e-06,  6.0339e-05,  1.0555e-04,  1.2152e-04, -4.6111e-05,\n",
      "         1.4685e-05, -1.8768e-04,  4.0593e-05, -1.7264e-04,  1.7323e-04,\n",
      "         3.1161e-05,  3.2302e-04,  2.2143e-05,  1.8449e-04,  1.8953e-05,\n",
      "        -1.7040e-04,  2.2369e-05,  6.4337e-05,  7.1069e-05,  7.7534e-05,\n",
      "         1.1895e-04, -3.2765e-05,  1.5540e-05,  5.5798e-05,  6.6247e-05,\n",
      "        -9.5693e-06,  8.4064e-05, -2.7219e-04,  1.5973e-05,  6.6944e-05,\n",
      "        -5.8380e-06,  7.4078e-05,  2.1738e-05,  1.5207e-05, -1.0149e-04,\n",
      "         2.1484e-06,  2.8277e-05,  5.5078e-05,  3.4190e-05,  5.2270e-05,\n",
      "         2.0608e-04, -2.2242e-05, -3.3874e-05,  1.8699e-04, -1.8353e-05,\n",
      "         1.1810e-05,  4.3819e-05,  1.1271e-04,  2.9767e-05, -3.7629e-05,\n",
      "         8.0570e-05,  3.7930e-05,  1.5840e-05,  9.8504e-05, -3.5293e-04,\n",
      "         1.2036e-04,  4.4371e-05, -1.1551e-05, -7.7640e-05,  1.5822e-05,\n",
      "         1.8507e-05,  2.1971e-05,  1.5946e-06, -2.5986e-04,  1.6483e-04,\n",
      "        -2.4486e-05, -1.5412e-04,  2.8608e-05, -1.3425e-04,  7.2606e-05,\n",
      "        -1.1840e-06,  8.5875e-05,  1.0008e-05,  7.1488e-06,  7.6288e-05,\n",
      "         5.9483e-05, -1.4920e-04, -9.8695e-06,  1.8502e-04,  4.6294e-05,\n",
      "         4.1772e-06,  2.4442e-05,  3.0878e-03,  3.2394e-03,  1.7390e-03,\n",
      "         3.3207e-03, -1.8927e-03, -2.2291e-03,  2.1438e-03,  2.3091e-03,\n",
      "         1.9081e-03, -4.3072e-03,  8.3127e-04,  1.3135e-04,  2.2026e-03,\n",
      "        -5.8163e-04,  9.0834e-04, -4.5654e-03, -1.4492e-03, -1.8372e-03,\n",
      "        -2.6219e-03,  2.2605e-03, -1.8955e-03, -2.8675e-03, -5.1041e-04,\n",
      "        -1.6374e-03, -3.4401e-03,  1.7703e-03, -1.7328e-03, -4.6836e-03,\n",
      "        -3.0863e-03, -4.2411e-03,  3.5436e-03, -3.0789e-03, -4.2542e-03,\n",
      "        -1.4297e-03, -1.4224e-03, -1.5520e-03, -2.9756e-03, -1.9807e-03,\n",
      "         1.8201e-03,  1.5108e-03,  2.8257e-03, -1.5186e-04,  2.2889e-03,\n",
      "         1.8071e-03, -1.3055e-03,  2.5275e-03,  3.7286e-03, -7.3930e-05,\n",
      "         4.4499e-03, -3.3035e-03, -3.5931e-03, -3.2273e-03, -2.7847e-03,\n",
      "        -9.5062e-05,  3.2300e-03,  8.8033e-05,  1.8844e-03, -2.1104e-03,\n",
      "         2.3256e-03, -1.6774e-03,  3.2570e-03,  1.1128e-03, -2.8047e-03,\n",
      "        -5.8791e-04, -2.2707e-03, -4.2880e-03, -3.6922e-03, -6.2336e-05,\n",
      "        -2.4633e-03,  3.1078e-03, -1.8067e-03, -3.7166e-03,  1.7649e-03,\n",
      "        -3.0051e-03, -2.6666e-03,  2.9630e-03, -2.3102e-03, -2.2262e-03,\n",
      "        -1.3635e-03, -3.2817e-03, -2.5955e-03, -2.2068e-03,  9.9399e-04,\n",
      "        -5.2490e-04,  2.7176e-03, -4.2136e-04,  2.6827e-03,  1.3604e-03,\n",
      "         1.8314e-03,  2.2777e-03, -2.6215e-03, -4.3920e-03,  2.8215e-03,\n",
      "         1.8234e-03,  3.4828e-03, -2.4156e-03,  3.1604e-03, -2.3794e-03,\n",
      "        -1.7175e-03, -1.4650e-03, -2.7010e-03, -5.5992e-04,  2.7730e-03,\n",
      "        -2.2287e-03,  2.6287e-03, -2.3675e-03, -1.3957e-03, -4.3583e-03,\n",
      "         2.1671e-03, -1.1075e-03,  3.2280e-03, -4.0298e-03,  3.2576e-03,\n",
      "        -1.5680e-03,  9.7481e-04, -2.5399e-03, -1.6038e-03,  3.8469e-04,\n",
      "        -2.8218e-03, -3.2701e-03,  3.0887e-03,  7.0741e-04,  3.0639e-03,\n",
      "        -1.9960e-03, -3.9957e-03, -4.7616e-04, -8.8848e-04, -1.7415e-04,\n",
      "        -1.9361e-03,  1.3159e-03, -3.3577e-03, -3.3719e-03, -4.3829e-03,\n",
      "         1.3534e-03, -7.4788e-04,  4.1228e-03,  7.1386e-04,  2.1059e-03,\n",
      "        -3.5043e-06,  2.0093e-03,  2.3666e-03, -5.3726e-04,  1.0310e-03,\n",
      "         2.5993e-03,  2.0402e-03,  1.7225e-03,  1.5664e-03,  2.5521e-03,\n",
      "        -1.2599e-03,  2.4784e-03, -1.2808e-03, -1.8662e-03, -5.3039e-04,\n",
      "         2.8348e-03, -3.4748e-03, -1.7796e-03,  1.8794e-03,  9.3958e-04,\n",
      "        -2.9343e-03,  2.9781e-03, -2.0350e-03, -2.9669e-03, -3.1087e-03,\n",
      "         2.1419e-03,  7.6575e-04,  2.0166e-03, -1.8433e-03,  2.0059e-03,\n",
      "        -4.8362e-04, -2.0790e-03,  1.1705e-03, -2.1469e-03,  2.1538e-03,\n",
      "         2.4997e-03, -2.1632e-03, -2.1128e-03,  3.5945e-03, -3.0707e-03,\n",
      "         2.7447e-03,  5.0161e-03,  2.4445e-03,  4.2453e-04,  6.0197e-04,\n",
      "        -9.5360e-04,  1.6757e-03, -1.6417e-03,  7.7923e-04,  3.5042e-03,\n",
      "        -8.6750e-05, -1.5407e-03,  1.9227e-03, -5.2551e-04, -2.5874e-03,\n",
      "        -4.7903e-04, -1.7781e-03, -3.8962e-03,  1.6939e-03,  9.7377e-04,\n",
      "         1.1308e-03, -8.0233e-04, -3.1209e-03,  1.4794e-05, -4.7909e-03,\n",
      "        -8.1129e-04,  4.2255e-03, -2.4823e-03,  3.3087e-03, -1.2974e-03,\n",
      "         2.4770e-03, -7.6315e-04, -2.6798e-03,  1.2252e-03, -1.4987e-03,\n",
      "        -1.2318e-04,  2.4246e-03,  1.0800e-03, -2.5536e-03,  3.6212e-03,\n",
      "        -2.4403e-03,  2.2650e-03, -3.0189e-03, -1.3839e-03, -9.4271e-04,\n",
      "        -1.9591e-03,  1.9959e-03,  2.8797e-03, -3.2006e-03,  2.2229e-03,\n",
      "         2.1297e-03, -3.8902e-03,  3.3863e-03, -1.7646e-03,  2.2470e-03,\n",
      "         7.5507e-04, -8.7339e-04,  2.0567e-03, -1.7240e-03,  2.0428e-03,\n",
      "        -3.1222e-03,  7.4732e-04, -2.9523e-03, -3.4304e-03,  3.0460e-03,\n",
      "         4.5491e-04,  3.6325e-03, -1.9707e-03,  2.6414e-03,  2.5923e-03,\n",
      "         2.2542e-03,  3.0966e-03, -2.8424e-03, -7.2893e-04, -2.3636e-03,\n",
      "        -1.9574e-03,  3.7926e-03, -9.3564e-04])\n",
      "tensor([-4.4940e-05,  1.0569e-05, -8.4355e-05,  9.9476e-05, -8.7722e-05,\n",
      "        -2.8200e-05, -3.2297e-05, -9.3338e-05, -5.5055e-05,  1.5031e-04,\n",
      "        -7.7377e-07,  3.1142e-05,  2.1239e-05, -4.6479e-05,  1.1683e-06,\n",
      "        -3.8748e-05,  3.3702e-05,  5.8185e-05, -1.4751e-04, -8.7497e-06,\n",
      "        -5.5863e-06, -2.0057e-05, -1.2148e-05, -5.8650e-05, -2.9978e-05,\n",
      "        -4.7708e-05, -2.5155e-06,  1.5070e-04,  1.6763e-04,  1.6638e-04,\n",
      "         7.7958e-05,  3.1045e-05, -6.2406e-05, -7.2028e-05, -1.0941e-05,\n",
      "        -1.9533e-05,  1.3366e-04,  4.7202e-06, -7.1393e-05, -1.2094e-05,\n",
      "        -8.5451e-05, -6.6477e-05,  8.1280e-05, -3.3549e-05, -9.0872e-05,\n",
      "         8.7019e-06, -1.7641e-05, -4.2387e-05,  1.9050e-05,  9.9676e-05,\n",
      "        -1.0925e-04, -5.0248e-05, -1.3581e-04, -3.1640e-05,  6.4015e-05,\n",
      "        -2.6906e-05, -5.9743e-05, -1.1454e-04,  4.5395e-06, -2.4989e-05,\n",
      "         1.1679e-05, -9.1390e-05,  9.9346e-05,  4.8448e-05,  8.4681e-05,\n",
      "        -2.8974e-05,  2.5201e-05,  8.1267e-05, -2.5705e-05, -7.9922e-05,\n",
      "         5.8160e-05,  8.6072e-05, -7.2100e-06,  8.3941e-05, -2.0054e-05,\n",
      "         2.8906e-04,  3.4538e-04,  6.4040e-05,  3.9240e-05,  1.2746e-04,\n",
      "         5.5391e-05, -1.2010e-04, -7.3884e-06,  1.1424e-06,  1.8147e-05,\n",
      "        -2.1857e-05, -8.3965e-05, -7.7176e-05,  4.1040e-05, -1.7513e-04,\n",
      "        -5.3501e-05, -8.5210e-05,  4.1892e-05, -1.2372e-04, -3.2549e-05,\n",
      "        -8.9010e-05,  1.7579e-04,  3.4402e-04,  1.2064e-04, -7.6907e-05,\n",
      "         1.2413e-05, -3.5999e-05,  2.4859e-05,  1.5614e-04, -5.8310e-05,\n",
      "         5.6666e-05,  4.8463e-05, -5.4153e-05,  3.3889e-05, -5.7075e-05,\n",
      "        -2.1648e-04,  4.4164e-04, -6.4053e-05, -1.0245e-04, -7.1220e-05,\n",
      "        -7.4460e-05, -7.8577e-05, -2.4789e-06, -3.5785e-05, -3.3611e-05,\n",
      "         5.7549e-05, -7.0507e-05,  2.8371e-04,  1.0646e-04, -9.9928e-05,\n",
      "         7.8183e-05, -5.0416e-05, -8.5705e-05, -1.5738e-06, -5.9043e-05,\n",
      "        -2.7875e-05,  8.0019e-05,  5.6807e-05,  3.2584e-05,  1.3149e-04,\n",
      "         1.2366e-04, -3.1577e-05,  3.0216e-05,  7.1107e-05, -1.5245e-04,\n",
      "        -1.0138e-04, -8.6229e-05,  3.8285e-05,  7.6284e-06, -1.8683e-04,\n",
      "         5.1542e-05, -1.4017e-05,  6.5534e-05,  4.4244e-04,  9.8300e-05,\n",
      "         2.1292e-04, -1.1639e-04, -5.2424e-05, -8.7740e-05, -5.3674e-05,\n",
      "        -8.0788e-05, -7.0420e-05, -7.2365e-05,  9.4402e-05,  8.1243e-05,\n",
      "         4.8772e-05, -1.1911e-04, -6.6592e-05,  2.1474e-05, -3.7583e-05,\n",
      "        -4.1990e-05,  1.3365e-04,  4.6033e-05,  8.6344e-06,  8.6049e-05,\n",
      "         2.7408e-05, -4.2770e-05, -6.0252e-05, -1.4753e-05, -7.7562e-05,\n",
      "        -5.3632e-05,  8.5964e-05, -9.2872e-05,  1.0870e-04, -6.2211e-05,\n",
      "        -1.1416e-04, -3.9830e-05,  4.1621e-05,  1.6119e-05, -7.4442e-05,\n",
      "        -1.8543e-04, -8.6417e-05,  1.6106e-04,  5.9413e-06, -1.2298e-04,\n",
      "         1.8619e-05, -1.4289e-05, -7.8100e-05, -3.2138e-05, -1.0161e-04,\n",
      "         1.9038e-05,  6.6903e-05, -6.1537e-06, -2.3655e-05,  4.6543e-05,\n",
      "        -1.1537e-04, -2.6131e-05,  1.8132e-04, -4.6350e-05, -1.1078e-04,\n",
      "        -1.1021e-04, -1.4451e-04,  4.7866e-05, -1.4357e-04,  2.1471e-05,\n",
      "        -6.9613e-05,  6.1141e-05,  2.1653e-05, -5.9651e-05, -1.3711e-04,\n",
      "        -1.8071e-05, -4.1531e-05, -1.1425e-04,  9.0630e-05, -4.0402e-05,\n",
      "         5.3727e-05, -9.6530e-05, -3.7798e-05,  3.5229e-05, -1.3669e-04,\n",
      "        -1.2998e-04, -7.2292e-06, -1.1867e-04, -1.4405e-04, -1.8524e-04,\n",
      "        -1.2303e-04, -4.7256e-05, -8.7901e-06, -6.4120e-05, -2.2298e-05,\n",
      "         5.1934e-06,  5.6567e-05, -7.8603e-05,  3.8643e-05, -5.2167e-05,\n",
      "        -7.4163e-06,  4.7353e-05,  3.5501e-05, -5.7432e-05,  4.1658e-05,\n",
      "        -4.4634e-05, -1.2131e-04, -3.9175e-05, -1.0734e-04, -6.0614e-05,\n",
      "         3.2601e-05,  2.3880e-05,  3.2551e-05,  2.5870e-05,  6.6743e-05,\n",
      "        -9.8267e-06,  4.4715e-05,  1.1872e-04,  2.6000e-05, -7.2720e-05,\n",
      "         7.2974e-05,  4.3819e-05,  1.6716e-05,  1.4046e-04,  5.5840e-05,\n",
      "         1.0844e-04,  5.0002e-05,  1.2843e-05,  7.1421e-05,  5.1037e-05,\n",
      "        -2.5778e-04, -3.2401e-06,  7.4657e-05, -7.3295e-05,  8.6040e-05,\n",
      "        -1.3580e-04,  2.9263e-05,  7.6405e-06,  9.7492e-05,  9.3271e-05,\n",
      "         1.0308e-04,  5.0875e-05,  3.1882e-05, -7.6681e-05, -1.0023e-04,\n",
      "        -1.5793e-04,  4.3922e-05,  8.2025e-05,  1.3690e-04,  5.0509e-05,\n",
      "        -1.3535e-05, -1.3696e-05,  2.1991e-04,  1.1951e-04, -7.6999e-05,\n",
      "         4.8214e-05,  4.4460e-05, -4.9307e-05, -1.1900e-04, -5.1497e-05,\n",
      "        -1.4958e-06,  3.9446e-05,  8.0249e-06,  3.6854e-05,  1.1494e-04,\n",
      "         3.6308e-05, -2.8710e-06, -1.5536e-04,  6.0571e-05,  1.0555e-04,\n",
      "        -7.1221e-06,  4.4669e-05,  1.4042e-04,  1.5112e-04,  3.7135e-05,\n",
      "        -1.7020e-05,  8.5303e-05, -4.1947e-04,  6.6737e-06, -1.8768e-04,\n",
      "        -3.2117e-05,  2.3105e-04,  1.8909e-05,  2.3329e-04,  1.7562e-04,\n",
      "         2.0531e-05, -7.2576e-05,  8.5020e-05,  7.6542e-05,  4.9019e-05,\n",
      "        -3.1451e-05, -1.3011e-04, -3.1968e-04,  1.0124e-04,  1.3559e-04,\n",
      "        -5.6200e-05,  7.8084e-05,  1.0889e-04,  6.2399e-05,  3.0473e-05,\n",
      "         5.2748e-05,  6.3056e-05,  9.7195e-05,  2.3807e-05, -1.2659e-04,\n",
      "         1.0433e-04, -2.0369e-06,  2.3481e-04,  2.3660e-04,  3.1905e-05,\n",
      "        -3.3476e-04,  1.3137e-04,  1.1924e-04, -2.1276e-04, -4.3678e-05,\n",
      "         9.4878e-06,  1.3116e-04,  5.3278e-05,  9.9840e-05, -4.2434e-05,\n",
      "        -2.1307e-04,  9.1635e-05,  4.9100e-05,  1.0683e-04,  6.7403e-05,\n",
      "         1.0043e-04,  1.6226e-04,  1.1327e-07, -4.5144e-05, -2.6108e-04,\n",
      "         1.0672e-04,  1.2217e-04, -2.9533e-05,  3.9106e-05,  8.7364e-05,\n",
      "         4.3370e-06, -1.0837e-04,  1.1840e-04, -3.0864e-04, -1.8145e-04,\n",
      "         1.1104e-04, -1.1204e-04,  3.5340e-05,  1.3869e-05,  3.9101e-06,\n",
      "         4.3889e-06, -3.0353e-04,  1.5362e-04,  1.8657e-04,  4.0386e-05,\n",
      "        -2.5485e-04,  1.1104e-04, -4.2176e-05, -9.2154e-05,  7.2000e-05,\n",
      "         4.2183e-05,  1.4432e-04,  1.1908e-04, -6.4179e-05,  1.1978e-04,\n",
      "         9.2654e-05,  5.3275e-05,  9.9549e-05, -3.4264e-05,  4.4536e-04,\n",
      "        -9.0217e-05, -1.0813e-04,  2.2225e-05,  1.3247e-04, -4.5963e-06,\n",
      "        -3.6101e-05,  8.9559e-05,  9.3487e-05,  5.0301e-05, -5.8123e-05,\n",
      "        -5.0962e-05,  1.1221e-04,  5.1149e-05,  8.0766e-05,  5.3672e-05,\n",
      "         4.2995e-05, -2.6533e-05, -2.7509e-04,  3.9394e-04,  2.5607e-05,\n",
      "         2.5997e-05, -3.5512e-05,  7.2381e-05,  3.4997e-05,  4.0932e-05,\n",
      "         2.7893e-06,  6.0339e-05,  1.0555e-04,  1.2152e-04, -4.6111e-05,\n",
      "         1.4685e-05, -1.8768e-04,  4.0593e-05, -1.7264e-04,  1.7323e-04,\n",
      "         3.1161e-05,  3.2302e-04,  2.2143e-05,  1.8449e-04,  1.8953e-05,\n",
      "        -1.7040e-04,  2.2369e-05,  6.4337e-05,  7.1069e-05,  7.7534e-05,\n",
      "         1.1895e-04, -3.2765e-05,  1.5540e-05,  5.5798e-05,  6.6247e-05,\n",
      "        -9.5693e-06,  8.4064e-05, -2.7219e-04,  1.5973e-05,  6.6944e-05,\n",
      "        -5.8380e-06,  7.4078e-05,  2.1738e-05,  1.5207e-05, -1.0149e-04,\n",
      "         2.1484e-06,  2.8277e-05,  5.5078e-05,  3.4190e-05,  5.2270e-05,\n",
      "         2.0608e-04, -2.2242e-05, -3.3874e-05,  1.8699e-04, -1.8353e-05,\n",
      "         1.1810e-05,  4.3819e-05,  1.1271e-04,  2.9767e-05, -3.7629e-05,\n",
      "         8.0570e-05,  3.7930e-05,  1.5840e-05,  9.8504e-05, -3.5293e-04,\n",
      "         1.2036e-04,  4.4371e-05, -1.1551e-05, -7.7640e-05,  1.5822e-05,\n",
      "         1.8507e-05,  2.1971e-05,  1.5945e-06, -2.5986e-04,  1.6483e-04,\n",
      "        -2.4486e-05, -1.5412e-04,  2.8608e-05, -1.3425e-04,  7.2606e-05,\n",
      "        -1.1840e-06,  8.5875e-05,  1.0008e-05,  7.1489e-06,  7.6288e-05,\n",
      "         5.9483e-05, -1.4920e-04, -9.8695e-06,  1.8502e-04,  4.6294e-05,\n",
      "         4.1773e-06,  2.4442e-05,  1.6794e-03,  1.6059e-03,  9.0300e-04,\n",
      "         2.1061e-03, -1.0948e-03, -1.2703e-03,  1.0512e-03,  1.3892e-03,\n",
      "         1.0956e-03, -1.9266e-03,  3.1406e-04,  6.6348e-05,  1.2191e-03,\n",
      "        -2.1568e-04,  5.2344e-04, -2.3360e-03, -7.1802e-04, -8.3339e-04,\n",
      "        -1.2706e-03,  1.2017e-03, -9.6000e-04, -1.5778e-03, -2.3636e-04,\n",
      "        -7.6121e-04, -1.5946e-03,  9.0672e-04, -9.2429e-04, -2.7204e-03,\n",
      "        -1.6043e-03, -1.9208e-03,  1.7746e-03, -1.6155e-03, -2.1884e-03,\n",
      "        -8.4608e-04, -6.9348e-04, -7.9786e-04, -1.4790e-03, -1.0107e-03,\n",
      "         1.1204e-03,  8.8164e-04,  1.2695e-03, -1.1370e-04,  9.6768e-04,\n",
      "         8.4574e-04, -6.8742e-04,  1.3671e-03,  1.9960e-03, -7.4113e-05,\n",
      "         2.2132e-03, -1.5654e-03, -1.7369e-03, -1.3516e-03, -1.6454e-03,\n",
      "        -5.7102e-06,  1.4729e-03,  8.8434e-05,  8.5746e-04, -1.0962e-03,\n",
      "         1.3709e-03, -8.0784e-04,  1.6203e-03,  7.1395e-04, -1.4005e-03,\n",
      "        -2.8648e-04, -9.7481e-04, -2.1973e-03, -1.8135e-03, -4.0461e-04,\n",
      "        -1.1848e-03,  1.5128e-03, -8.4450e-04, -1.9055e-03,  9.2641e-04,\n",
      "        -1.7005e-03, -1.4125e-03,  1.6375e-03, -1.3453e-03, -1.2483e-03,\n",
      "        -6.0872e-04, -1.4830e-03, -1.5811e-03, -1.3067e-03,  5.5477e-04,\n",
      "        -2.1348e-04,  1.4711e-03, -2.5505e-04,  1.4793e-03,  7.7283e-04,\n",
      "         8.5897e-04,  1.2670e-03, -1.1039e-03, -2.2133e-03,  1.7791e-03,\n",
      "         1.0332e-03,  1.6407e-03, -1.1580e-03,  1.7471e-03, -1.5048e-03,\n",
      "        -9.4125e-04, -7.0696e-04, -1.4926e-03, -4.0302e-04,  1.3304e-03,\n",
      "        -1.1390e-03,  1.5396e-03, -1.0619e-03, -8.1570e-04, -1.9063e-03,\n",
      "         9.3487e-04, -5.9489e-04,  1.7129e-03, -1.5830e-03,  1.7438e-03,\n",
      "        -9.8375e-04,  4.4509e-04, -1.0044e-03, -8.3083e-04,  2.2142e-04,\n",
      "        -1.4706e-03, -1.7947e-03,  1.3634e-03,  4.3497e-04,  1.0236e-03,\n",
      "        -1.0025e-03, -2.0259e-03, -1.0502e-04, -3.7595e-04, -1.4356e-04,\n",
      "        -9.8320e-04,  6.2859e-04, -1.5742e-03, -1.6726e-03, -2.2513e-03,\n",
      "         6.2321e-04, -2.9303e-04,  1.7828e-03,  3.4963e-04,  1.1109e-03,\n",
      "        -4.6668e-05,  1.1937e-03,  9.4780e-04, -3.3695e-04,  2.7067e-06,\n",
      "         1.5293e-03,  1.0745e-03,  1.0052e-03,  8.1268e-04,  1.1688e-03,\n",
      "        -5.3314e-04,  1.0663e-03, -4.4285e-04, -1.0449e-03, -2.9981e-04,\n",
      "         1.5198e-03, -1.6659e-03, -8.1732e-04,  9.8462e-04,  5.1227e-04,\n",
      "        -1.2978e-03,  1.4495e-03, -1.0835e-03, -1.4530e-03, -1.7548e-03,\n",
      "         1.0471e-03,  4.4741e-04,  1.1290e-03, -8.8384e-04,  4.2542e-04,\n",
      "        -2.2512e-04, -1.1471e-03,  5.6107e-04, -1.1659e-03,  9.6217e-04,\n",
      "         1.2478e-03, -1.0895e-03, -1.1694e-03,  1.9080e-03, -1.6213e-03,\n",
      "         1.4895e-03,  2.4472e-03,  2.0718e-03,  1.9932e-04,  3.0357e-04,\n",
      "        -4.4334e-04,  7.4793e-04, -3.3982e-04,  4.8660e-04,  1.7010e-03,\n",
      "        -4.6315e-05, -7.0640e-04,  9.1042e-04, -2.8892e-04, -1.1985e-03,\n",
      "        -2.7766e-04, -9.9659e-04, -2.2169e-03,  9.3518e-04,  4.9247e-04,\n",
      "         6.9030e-04, -4.2751e-04, -1.6114e-03,  2.1288e-05, -2.0782e-03,\n",
      "        -4.4995e-04,  1.9808e-03, -1.3121e-03,  1.7718e-03, -8.1739e-04,\n",
      "         1.2137e-03, -4.4455e-04, -1.3125e-03,  6.2658e-04, -8.1577e-04,\n",
      "        -1.1504e-04,  1.1383e-03,  6.2231e-04, -1.0823e-03,  1.9295e-03,\n",
      "        -1.3997e-03,  1.2891e-03, -1.6988e-03, -8.0166e-04, -5.3609e-04,\n",
      "        -1.1008e-03,  1.0844e-03,  1.5639e-03, -1.6814e-03,  1.0314e-03,\n",
      "         1.5306e-03, -1.8952e-03,  1.7936e-03, -1.0479e-03,  1.3246e-03,\n",
      "         4.0164e-04, -4.6202e-04,  1.0619e-03, -7.7632e-04,  1.1209e-03,\n",
      "        -1.7679e-03,  3.9588e-04, -1.6710e-03, -1.6181e-03,  1.8829e-03,\n",
      "         1.8562e-04,  1.8056e-03, -9.4251e-04,  1.3446e-03,  1.3442e-03,\n",
      "         1.2408e-03,  1.3874e-03, -1.3579e-03, -3.7856e-04, -1.3190e-03,\n",
      "        -1.1178e-03,  2.2162e-03, -4.6341e-04])\n",
      "tensor([[-4.8940e-05, -3.0708e-05, -4.6988e-05,  ...,  3.9379e-05,\n",
      "         -1.3228e-05, -3.6366e-05],\n",
      "        [ 2.7413e-04,  2.5866e-04,  2.7720e-04,  ..., -2.8188e-04,\n",
      "          1.1133e-04,  2.7484e-04],\n",
      "        [ 1.2556e-03,  1.2872e-03,  1.2368e-03,  ..., -1.2827e-03,\n",
      "          9.5254e-04,  1.2565e-03],\n",
      "        ...,\n",
      "        [ 1.0450e-03,  9.8425e-04,  1.0536e-03,  ..., -1.0628e-03,\n",
      "          6.6099e-04,  1.0119e-03],\n",
      "        [-6.6336e-05, -4.5944e-05, -8.3429e-05,  ...,  8.3622e-05,\n",
      "          7.5801e-05, -7.8171e-05],\n",
      "        [ 7.6890e-04,  7.4316e-04,  8.1286e-04,  ..., -8.2160e-04,\n",
      "          4.3068e-04,  8.1363e-04]])\n",
      "tensor([ 2.8484e-05, -2.5660e-04, -1.2518e-03,  1.2748e-04,  2.8443e-04,\n",
      "        -8.6785e-04,  2.8018e-03,  8.0951e-04,  2.9032e-04, -2.7659e-03,\n",
      "        -6.9859e-05, -1.5033e-03,  9.4091e-04,  1.4808e-04, -8.0964e-04,\n",
      "         6.3450e-04, -8.2522e-04,  1.2231e-03,  3.6081e-05,  9.3221e-04,\n",
      "        -1.6200e-03,  1.5689e-04, -1.0951e-03, -6.6404e-04,  1.4919e-04,\n",
      "         1.2808e-03,  1.7925e-04, -1.3229e-04, -2.3966e-03, -8.5316e-06,\n",
      "         1.4836e-03, -2.8635e-04,  1.6988e-04, -6.9370e-04,  2.0943e-03,\n",
      "        -8.1690e-04,  3.6106e-04,  9.3135e-04,  6.5116e-05, -7.1480e-04,\n",
      "        -6.9809e-04, -2.0873e-05, -1.4213e-03,  5.3863e-05,  6.7184e-05,\n",
      "        -2.7176e-03, -5.1285e-04, -5.2683e-05,  3.2984e-04,  7.6263e-04,\n",
      "         4.0580e-04,  1.3335e-03,  3.6050e-04,  5.0347e-04, -1.7349e-03,\n",
      "        -1.8359e-04, -7.6592e-05, -1.8767e-03, -1.8008e-06,  9.9233e-05,\n",
      "        -7.6179e-05,  7.6060e-04, -1.5791e-04,  2.2042e-04,  1.1101e-03,\n",
      "        -1.5214e-04,  3.5358e-04,  8.6029e-04,  6.6444e-04,  2.6524e-04,\n",
      "        -7.4070e-04, -1.4236e-03,  9.3043e-05,  9.7933e-05, -1.0558e-03,\n",
      "         9.6922e-05, -3.1981e-04, -2.8210e-06, -2.3520e-04,  6.1172e-04,\n",
      "        -4.5258e-05, -6.9192e-04,  2.2823e-03,  2.8789e-03,  1.3929e-03,\n",
      "         1.4612e-04,  3.4890e-03,  1.5007e-04,  1.3994e-03,  4.1332e-04,\n",
      "         1.0966e-03,  7.9806e-04, -2.4483e-04, -5.6459e-04, -7.0072e-04,\n",
      "         1.0244e-04,  7.7295e-04,  3.2917e-04,  1.1299e-03,  1.6471e-03,\n",
      "         2.5127e-03,  3.0644e-04, -4.0798e-04,  3.0557e-04,  1.4053e-03,\n",
      "        -8.4194e-05,  1.1687e-03,  4.2556e-04,  1.1011e-04, -2.2015e-03,\n",
      "         1.4464e-03, -3.2527e-03, -5.7397e-04,  1.6756e-03, -2.0634e-03,\n",
      "        -3.6032e-04, -8.7934e-05, -1.7127e-03,  1.0172e-03, -1.5798e-03,\n",
      "        -7.4298e-04, -2.2053e-03, -3.4774e-04,  9.7263e-04,  4.3697e-04,\n",
      "         3.5107e-05,  1.4627e-03,  3.0446e-04,  2.2117e-03,  3.9273e-04,\n",
      "        -6.9978e-04, -1.8822e-05,  8.4310e-04,  3.5274e-05, -1.1207e-03,\n",
      "         6.4098e-04,  2.7479e-03,  1.2248e-03,  1.5592e-04, -2.7345e-03,\n",
      "         9.1596e-04, -8.7259e-04, -3.4220e-04, -3.1445e-05, -1.2445e-04,\n",
      "         1.0878e-03,  1.2699e-03,  4.5337e-04,  2.3443e-03, -1.0885e-04,\n",
      "        -6.3900e-04,  1.0625e-03,  4.9522e-04, -9.3584e-04, -2.8286e-04,\n",
      "        -5.3795e-04,  1.9633e-04,  1.6435e-04,  7.6718e-04, -1.4571e-03,\n",
      "        -4.9014e-04, -1.9239e-07, -2.0928e-05, -2.4487e-05, -5.4495e-04,\n",
      "         2.4645e-05, -6.0043e-05,  1.2875e-03,  1.0409e-03, -1.3572e-04,\n",
      "        -1.1730e-04, -1.4323e-03,  5.5400e-04,  8.3045e-05,  1.9547e-03,\n",
      "        -8.6185e-04, -5.5229e-05, -3.0526e-05, -1.0638e-05, -4.7202e-04,\n",
      "         1.1079e-03, -1.5149e-04, -4.0688e-03,  1.4176e-03, -3.1086e-04,\n",
      "         8.3250e-05,  3.5567e-05, -4.3412e-04,  4.9350e-04, -8.2945e-04,\n",
      "        -1.0124e-04, -1.5916e-03, -3.6120e-05,  1.3818e-03, -1.8895e-04,\n",
      "        -1.7499e-04,  1.7810e-03,  3.9495e-04,  1.3646e-03,  1.6058e-04,\n",
      "         6.3527e-05, -7.6960e-04,  8.8247e-04,  9.0259e-05, -6.7345e-04,\n",
      "        -2.7106e-04,  1.6394e-04, -8.7086e-04, -9.9115e-05,  7.3490e-04,\n",
      "        -1.3097e-03,  1.3752e-03,  6.0780e-04, -9.5084e-05, -2.9074e-04,\n",
      "        -1.8762e-03, -5.0663e-05, -1.1386e-03, -1.2734e-04,  1.2406e-04,\n",
      "         2.7299e-03, -7.6064e-05,  1.0481e-04, -5.9866e-04,  1.9346e-06,\n",
      "         1.0653e-04,  1.9710e-03,  5.2511e-05, -4.0755e-04,  2.1995e-03,\n",
      "         4.3100e-05,  1.8077e-04,  7.5142e-05, -3.1479e-04,  1.2185e-04,\n",
      "         1.0854e-03,  2.8565e-04, -1.6307e-03, -1.6062e-04,  6.4003e-04,\n",
      "         2.0831e-04, -7.8318e-04,  7.6992e-04,  1.2850e-03,  8.8854e-04,\n",
      "         1.6625e-04,  1.6450e-03, -5.7597e-05,  9.0365e-05,  9.5135e-04,\n",
      "         1.6634e-04, -2.2900e-03,  1.6685e-04, -1.0774e-03,  1.0032e-04,\n",
      "        -9.3514e-04])\n",
      "tensor([[ 1.4473e-06,  9.6684e-06,  1.9856e-05,  ...,  7.1575e-06,\n",
      "         -9.7020e-06, -4.6808e-06],\n",
      "        [-5.4949e-06,  2.3826e-05, -1.2652e-05,  ..., -9.3041e-07,\n",
      "         -5.9968e-06,  9.1810e-08],\n",
      "        [ 4.5669e-06,  9.9960e-06, -1.1067e-05,  ..., -2.5877e-05,\n",
      "          9.8246e-06,  3.3792e-05],\n",
      "        ...,\n",
      "        [ 6.6459e-05, -7.7956e-05, -4.1408e-05,  ...,  9.0253e-05,\n",
      "          9.6421e-05, -5.8874e-05],\n",
      "        [-2.2413e-05,  1.4018e-05, -3.3325e-06,  ..., -3.5195e-05,\n",
      "         -3.8672e-05, -3.0034e-06],\n",
      "        [ 6.8014e-06, -1.8986e-05, -2.2637e-05,  ...,  2.9291e-06,\n",
      "          9.5354e-06, -5.9610e-06]])\n",
      "tensor([[ 2.6744e-06, -2.8458e-06, -1.3563e-06,  ...,  7.8127e-06,\n",
      "         -1.3807e-05,  1.3526e-05],\n",
      "        [ 1.3060e-05, -4.5732e-06, -4.1333e-06,  ...,  3.0358e-05,\n",
      "          8.1637e-06,  2.4978e-05],\n",
      "        [ 3.9968e-05, -1.1280e-04, -1.0717e-04,  ...,  2.7738e-05,\n",
      "         -5.8038e-05,  1.2580e-04],\n",
      "        ...,\n",
      "        [-5.1052e-05,  1.4848e-06, -1.3656e-06,  ..., -2.8929e-05,\n",
      "          2.6287e-05, -2.9454e-05],\n",
      "        [-1.7129e-06,  9.4492e-07,  5.0102e-07,  ..., -3.0399e-06,\n",
      "          3.9931e-06, -5.4608e-06],\n",
      "        [-7.3914e-06,  4.2131e-06,  2.4704e-06,  ..., -1.0265e-05,\n",
      "          1.6622e-05, -2.0336e-05]])\n",
      "tensor([ 1.5263e-05,  1.0677e-05,  4.3419e-05,  2.0235e-05, -8.9179e-07,\n",
      "         4.0135e-06,  4.3888e-06,  1.9684e-05, -2.0881e-05,  9.3023e-05,\n",
      "         2.5746e-05, -1.3940e-05,  1.4741e-05,  5.8321e-06, -9.0960e-06,\n",
      "        -3.6011e-06,  4.0895e-05,  2.0791e-05, -1.3063e-05, -3.9283e-06,\n",
      "        -1.0901e-05, -4.8251e-05, -1.6542e-05,  3.8408e-06, -1.4834e-05,\n",
      "         6.4078e-06,  1.7828e-06, -2.0738e-05, -1.8394e-05,  1.9318e-05,\n",
      "        -1.0414e-05, -5.5441e-06, -8.1529e-06,  1.2198e-06, -5.3851e-07,\n",
      "        -1.8823e-06, -1.1292e-05, -8.4057e-06,  4.4713e-06,  2.4680e-06,\n",
      "         9.3794e-06, -1.4007e-05, -5.6931e-06, -4.0454e-05,  4.3851e-05,\n",
      "         6.1233e-06,  3.5969e-05, -1.5511e-05,  5.9517e-06,  1.2169e-04,\n",
      "         4.2475e-05,  3.3735e-05, -5.6646e-06,  1.3964e-04,  8.5922e-06,\n",
      "         3.6636e-06,  1.3441e-05, -2.0611e-05, -3.0478e-05, -5.1082e-06,\n",
      "        -9.6205e-06,  1.6288e-05, -2.6611e-05,  6.7639e-05, -2.8135e-05,\n",
      "        -3.8266e-05, -3.1154e-05,  9.8901e-07,  5.3889e-05,  2.6624e-05,\n",
      "        -6.8203e-05, -1.2075e-04,  9.4322e-06, -9.8882e-06, -1.7912e-05,\n",
      "         3.5107e-06, -1.6035e-05, -1.3541e-05, -1.1440e-06, -4.4940e-05,\n",
      "         1.4290e-05, -1.5295e-05, -1.5501e-05, -3.3103e-05,  8.9247e-05,\n",
      "         7.7616e-05, -1.0010e-05, -2.0305e-06, -4.7462e-06, -1.2766e-04,\n",
      "        -8.0178e-06,  3.4036e-06,  5.7867e-06,  8.1794e-06, -9.2285e-06,\n",
      "         1.1070e-06, -1.4535e-05,  5.6030e-05,  1.8184e-05,  1.2684e-05,\n",
      "        -3.6469e-05, -1.3368e-05, -3.7054e-04,  4.7672e-05,  8.1853e-06,\n",
      "        -4.7665e-05,  9.2846e-05,  1.9834e-05, -4.4049e-06, -1.4272e-07,\n",
      "         8.7826e-06, -1.2901e-05, -1.6625e-05, -2.2718e-06,  3.1114e-05,\n",
      "         2.0147e-05,  7.3631e-06, -1.2900e-05,  1.1390e-05,  9.0955e-06,\n",
      "         3.6225e-05, -1.8709e-05,  2.3320e-05,  1.0087e-06, -1.3740e-05,\n",
      "         4.4956e-05, -4.7125e-06, -4.0257e-04, -1.7307e-05, -1.6551e-05,\n",
      "         4.5725e-05, -1.9404e-05,  4.6613e-05,  6.1360e-06,  5.8740e-05,\n",
      "        -1.3411e-06,  3.2362e-05, -2.2883e-05, -3.7427e-05, -3.4377e-05,\n",
      "         5.6606e-04,  8.9922e-06, -1.8352e-05, -1.8087e-05, -1.2084e-05,\n",
      "         1.8915e-06, -2.7572e-05, -4.3583e-05,  5.3643e-04, -1.5114e-05,\n",
      "         1.9102e-05, -6.3662e-07,  7.2328e-06, -1.0887e-05,  1.4136e-05,\n",
      "        -2.3688e-05, -2.2531e-05, -2.3523e-05, -1.1884e-04,  1.8460e-05,\n",
      "        -1.7613e-05, -2.2992e-05,  5.1504e-05, -1.9987e-05, -9.7811e-05,\n",
      "        -2.4212e-05, -1.3219e-05,  8.9993e-06, -2.5880e-05, -9.4469e-06,\n",
      "         8.8795e-06,  4.4109e-05,  1.5214e-04, -3.3033e-07, -2.9437e-05,\n",
      "         3.6250e-05, -3.9730e-05,  2.3890e-04,  2.6127e-05, -4.9313e-05,\n",
      "         8.9386e-05,  2.3116e-05,  1.8861e-06,  2.8282e-05, -1.8039e-05,\n",
      "        -1.9093e-05, -1.2735e-05, -8.7705e-06,  1.1907e-05, -3.1451e-05,\n",
      "        -7.2752e-06,  9.7065e-05,  2.5405e-05,  1.8190e-05,  5.8336e-06,\n",
      "         9.7113e-06, -1.5032e-05,  2.5965e-05,  2.1651e-06, -2.0602e-05,\n",
      "         5.4472e-05, -4.1762e-05,  7.3128e-06, -3.7616e-05, -5.3985e-06,\n",
      "         1.5269e-05, -4.3047e-06, -3.4032e-05,  4.3122e-06, -2.1645e-05,\n",
      "         7.0304e-06, -1.1588e-05, -8.3923e-06,  2.7504e-06, -1.7545e-07,\n",
      "        -3.0760e-06, -2.3227e-05, -4.8807e-05, -2.8904e-05, -5.5143e-05,\n",
      "         1.3771e-05, -3.9055e-05, -1.5586e-05,  6.6485e-05,  7.7931e-06,\n",
      "         2.8930e-06, -2.7988e-06,  4.4006e-05, -3.7710e-05, -8.5345e-07,\n",
      "        -1.8265e-05,  1.9092e-04, -4.6124e-05, -4.2579e-05,  4.6621e-04,\n",
      "        -4.4342e-06, -2.7422e-05,  4.2942e-06,  3.8952e-06, -2.3523e-05,\n",
      "        -8.7192e-06, -3.4426e-05,  6.8796e-07, -2.1650e-05, -5.9742e-06,\n",
      "         1.1675e-05,  1.3853e-05, -9.7511e-06,  1.6634e-05, -1.6007e-05,\n",
      "         6.5734e-05, -1.7119e-05,  2.7955e-04, -1.0354e-05,  3.8515e-06,\n",
      "        -8.3261e-06, -1.8448e-05,  5.8793e-04,  1.2295e-03, -3.9355e-04,\n",
      "        -3.6408e-04, -3.6279e-05, -1.7054e-04,  1.9273e-04, -4.6065e-04,\n",
      "         1.7174e-03, -8.5749e-04, -4.6516e-05, -1.9992e-04,  2.8590e-04,\n",
      "        -2.7377e-05, -4.4514e-04, -2.1732e-04, -1.5953e-04, -7.6496e-06,\n",
      "         1.7465e-05,  5.6219e-04,  7.5752e-04, -1.4239e-04, -2.8395e-04,\n",
      "         1.3071e-05,  1.0395e-05,  2.6471e-04, -5.9976e-05,  7.6969e-05,\n",
      "         8.8389e-04,  2.0446e-03,  1.6952e-04,  1.8597e-04,  2.8382e-06,\n",
      "        -2.9961e-05, -2.9347e-04,  3.1646e-05,  4.3845e-05,  3.4000e-05,\n",
      "         1.5423e-05, -1.1371e-04,  1.2385e-04, -1.5836e-05, -7.0597e-06,\n",
      "         1.1610e-05, -1.0793e-04, -1.0389e-04, -1.0342e-04,  2.3572e-04,\n",
      "         7.4253e-04, -6.4952e-04, -5.7230e-05,  4.9605e-05,  3.5674e-04,\n",
      "        -1.1852e-04,  4.7629e-04,  1.9357e-04,  3.1397e-06, -1.1688e-04,\n",
      "        -1.3396e-04,  8.0210e-05, -4.8786e-04, -2.7494e-05,  4.1516e-04,\n",
      "         7.0665e-04, -4.3115e-06,  5.0491e-04,  3.4589e-05,  4.3583e-04,\n",
      "        -8.7129e-04, -1.1350e-03,  1.7782e-03,  1.6186e-05,  9.8393e-05,\n",
      "         1.5237e-03,  1.0522e-04, -3.5556e-05, -6.6953e-05,  2.5891e-05,\n",
      "         2.4157e-05,  2.1796e-05, -3.0594e-04, -1.7709e-04,  7.2302e-06,\n",
      "         3.8227e-04, -6.6003e-04, -1.4119e-05,  2.4967e-04, -3.8511e-04,\n",
      "        -6.5886e-04, -1.2935e-05, -9.9513e-04, -3.6861e-04,  4.9401e-04,\n",
      "        -2.6560e-04,  6.1722e-04, -3.0867e-04,  1.6216e-04,  8.0750e-05,\n",
      "         1.6566e-05,  4.3146e-03,  1.1750e-05,  2.0067e-03, -4.0113e-04,\n",
      "        -6.7866e-05,  3.0505e-05,  5.1441e-04, -9.5084e-05, -9.1345e-05,\n",
      "        -1.3085e-05, -2.5271e-05, -1.9905e-05,  5.0247e-04,  2.0958e-03,\n",
      "        -1.8148e-04,  6.6638e-04, -1.6676e-06, -8.2090e-05, -1.2036e-04,\n",
      "        -2.4660e-04, -3.9634e-05,  4.6021e-05, -4.3416e-05,  1.9897e-05,\n",
      "         5.6764e-04,  5.2828e-05,  1.9467e-03, -1.7217e-04, -2.5008e-05,\n",
      "        -3.0432e-04,  2.3812e-04,  1.1334e-05,  8.5874e-04,  2.6719e-07,\n",
      "         4.4122e-05, -5.0305e-04, -8.2589e-05,  4.8976e-03,  2.6455e-05,\n",
      "         2.2120e-03,  9.1662e-04,  3.8936e-05,  1.4018e-03,  8.2187e-05,\n",
      "        -1.1527e-04, -3.3419e-05, -8.3752e-05, -1.6292e-04,  3.6862e-03,\n",
      "        -9.4792e-08, -1.2765e-04,  6.7167e-04,  1.8862e-05, -2.0404e-04,\n",
      "         2.0411e-04, -3.6590e-04, -4.2594e-04,  2.3194e-04, -4.0925e-04,\n",
      "         8.4455e-05,  7.7808e-05,  4.0347e-06,  8.6827e-04,  6.1862e-04,\n",
      "        -3.9078e-04,  5.8439e-05, -1.8609e-05,  2.6422e-06, -1.1817e-05,\n",
      "        -6.5577e-05,  3.3912e-04,  5.0011e-04,  2.1796e-03, -4.7764e-06,\n",
      "        -6.9971e-05, -1.3561e-04, -3.2455e-04, -4.7646e-04, -1.4334e-05,\n",
      "         2.2859e-03,  9.4962e-04,  5.4352e-05, -1.6623e-05, -9.1089e-05,\n",
      "         1.3631e-04, -1.0683e-06,  1.2822e-05, -1.2589e-04, -1.6874e-05,\n",
      "         4.8645e-04,  5.8831e-04, -2.1525e-04,  1.7046e-04,  2.2147e-04,\n",
      "         9.7029e-05, -1.0835e-04,  2.1833e-05,  5.3221e-04,  1.7423e-05,\n",
      "         5.6180e-06, -7.1509e-06,  1.3560e-04,  8.4806e-06, -4.2440e-05,\n",
      "         2.1661e-05,  1.3891e-03, -5.1648e-05, -4.9862e-04,  9.8611e-05,\n",
      "         3.8533e-04, -1.1975e-06, -6.0577e-05,  5.3252e-05, -2.8512e-06,\n",
      "         9.3808e-05, -3.6528e-06,  3.4282e-05,  1.1277e-03, -5.1905e-05,\n",
      "        -9.7135e-05, -6.4639e-05,  1.5596e-05, -1.0423e-04, -3.8127e-04,\n",
      "         8.8865e-05,  6.7241e-04,  8.9034e-06, -1.3456e-04,  5.6449e-04,\n",
      "        -4.1174e-04, -1.2648e-03, -4.1823e-05,  8.4879e-04,  2.2028e-04,\n",
      "         5.4514e-06,  6.7427e-04, -3.4584e-05, -1.1144e-04,  7.4218e-06,\n",
      "         4.3218e-04, -9.0047e-05,  1.1798e-03, -2.1201e-04, -2.8882e-05,\n",
      "        -1.1304e-04,  4.0252e-04, -4.2353e-05,  1.2846e-05, -1.7361e-04,\n",
      "         3.2679e-04, -1.9586e-04,  3.8608e-04, -2.9757e-04,  7.0321e-04,\n",
      "         1.7172e-04, -3.7375e-05,  6.7106e-05, -2.4754e-04, -9.2683e-04,\n",
      "         1.5662e-04,  2.5793e-06,  1.5848e-05, -8.3189e-06,  8.7308e-05,\n",
      "        -1.6772e-04, -6.1192e-04, -4.3604e-04,  7.1305e-05,  4.3127e-05,\n",
      "        -4.2929e-05, -2.7741e-05,  1.7113e-05,  6.0279e-05, -1.1989e-04,\n",
      "        -7.3242e-05,  1.1518e-05,  7.9033e-05, -3.1981e-04, -6.9051e-05,\n",
      "         6.7154e-05, -8.1708e-05,  1.9985e-05, -1.2424e-05,  8.6494e-05,\n",
      "         1.2192e-04, -9.4194e-05, -6.1621e-05,  2.5361e-05,  4.1799e-05,\n",
      "         6.6754e-06,  6.0486e-06,  1.8345e-05, -5.7464e-05, -5.0872e-05,\n",
      "        -4.4895e-06,  7.5991e-05,  7.4625e-05,  4.6187e-05, -9.2415e-06,\n",
      "        -1.0713e-04,  7.0943e-04,  2.7443e-05, -1.0518e-04, -1.4182e-04,\n",
      "        -2.1174e-05,  4.8832e-04,  1.6340e-04, -1.9840e-04, -2.4964e-05,\n",
      "        -3.9317e-04,  5.2159e-05,  1.8441e-05, -4.4027e-05,  1.2019e-04,\n",
      "        -1.7206e-04, -1.5158e-05, -2.1018e-05, -4.7391e-05,  7.2826e-05,\n",
      "        -2.1679e-04,  1.1392e-04,  1.8046e-04,  7.5405e-05, -4.6749e-06,\n",
      "         1.7079e-04,  2.3743e-04, -4.8721e-04, -3.1310e-04,  6.1274e-05,\n",
      "         3.1753e-05,  6.5158e-05,  2.0203e-06, -6.4332e-05,  4.9308e-05,\n",
      "        -2.7700e-06,  1.7166e-04,  3.0188e-05, -6.0228e-05,  8.1051e-05,\n",
      "        -1.2598e-04, -4.9810e-04,  1.3061e-04,  2.6470e-05,  1.1843e-05,\n",
      "        -2.7676e-05,  3.0598e-04, -5.7646e-05,  8.9983e-04,  8.7313e-05,\n",
      "         2.6314e-05,  3.8803e-05, -3.8163e-05,  4.7709e-05, -2.0894e-04,\n",
      "         2.0731e-04,  4.8118e-05,  7.0424e-04,  5.4479e-05, -2.9333e-03,\n",
      "         3.7848e-05, -6.2832e-05,  1.0479e-04, -1.6978e-04, -6.8064e-05,\n",
      "        -1.7311e-05, -6.1386e-07, -3.4299e-05, -6.1492e-05,  5.4020e-05,\n",
      "        -1.1052e-05,  9.8426e-05, -1.0634e-04, -1.6237e-03, -5.0242e-05,\n",
      "        -7.3491e-06,  9.2667e-05, -4.4134e-06,  6.8598e-05,  5.2930e-05,\n",
      "         9.5008e-06, -7.1489e-05,  1.7289e-04, -1.2876e-05, -2.5552e-03,\n",
      "         8.3306e-05, -1.0922e-04, -2.0232e-04, -6.6678e-04,  1.7452e-04,\n",
      "        -2.7825e-04,  2.1440e-04, -7.3040e-06, -1.5217e-04,  2.3384e-04,\n",
      "        -1.8955e-04,  1.1590e-04,  1.6610e-03,  1.9748e-05,  4.2489e-05,\n",
      "        -1.1803e-04, -3.6749e-05, -1.3602e-05,  1.1117e-04, -3.1810e-04,\n",
      "         2.5806e-03, -9.2742e-05, -6.8013e-05,  9.9106e-05,  3.1672e-05,\n",
      "         4.3203e-05, -4.5845e-05, -1.0315e-04,  2.1721e-04, -9.7467e-05,\n",
      "         7.7247e-04, -4.9102e-05, -3.7911e-05,  9.7182e-05,  5.7679e-04,\n",
      "        -7.2530e-05, -1.1007e-03, -1.9151e-05,  5.0222e-05, -4.1747e-05,\n",
      "        -9.3480e-05, -4.6852e-05, -3.6167e-05,  1.9837e-04,  1.3412e-03,\n",
      "        -1.4640e-06,  8.9988e-05, -2.0317e-04, -1.5225e-04,  6.2427e-04,\n",
      "        -8.9295e-05,  7.4461e-04,  7.4859e-04, -2.2504e-04,  1.0929e-05,\n",
      "        -1.2701e-04,  4.4780e-05, -1.2404e-04, -5.1576e-05, -7.5439e-04,\n",
      "         6.9166e-05,  1.3119e-04, -3.6365e-05,  2.8853e-04, -6.4991e-05,\n",
      "         1.4351e-04, -1.9415e-05,  4.5002e-05,  4.7715e-05,  8.3530e-05,\n",
      "        -1.4267e-05,  8.1583e-05, -1.7464e-04, -1.4393e-04,  4.4888e-05,\n",
      "         1.4260e-04,  1.3723e-05, -1.0713e-04,  4.3385e-05, -2.0772e-04,\n",
      "         1.5686e-03,  9.4015e-05, -4.5304e-05,  5.4037e-05,  4.8858e-05,\n",
      "        -1.7180e-05, -8.1878e-06,  1.3042e-05,  7.6919e-05, -1.8822e-04,\n",
      "        -1.0718e-04,  1.6766e-04, -5.8403e-05, -2.4627e-04,  9.9132e-05,\n",
      "        -3.6196e-04,  4.0727e-05, -6.5028e-06, -1.4210e-05, -2.1188e-04,\n",
      "         2.0082e-06, -8.4621e-05, -3.5025e-04, -8.0175e-05, -3.0975e-05,\n",
      "        -1.7784e-04,  5.8386e-04, -3.0199e-05,  1.5385e-04,  1.3511e-05,\n",
      "        -6.8491e-05,  2.3020e-04,  5.1525e-06, -1.0909e-04,  4.9811e-06,\n",
      "         6.8662e-05, -1.2180e-05, -7.6729e-04, -4.7410e-05, -5.1189e-05,\n",
      "         9.5663e-05, -5.6250e-05,  3.7716e-04,  6.7647e-05, -3.5372e-04,\n",
      "        -3.9198e-05, -8.0293e-06, -2.9882e-05])\n",
      "tensor([ 1.5263e-05,  1.0677e-05,  4.3419e-05,  2.0235e-05, -8.9179e-07,\n",
      "         4.0135e-06,  4.3888e-06,  1.9684e-05, -2.0881e-05,  9.3023e-05,\n",
      "         2.5746e-05, -1.3940e-05,  1.4741e-05,  5.8321e-06, -9.0960e-06,\n",
      "        -3.6011e-06,  4.0895e-05,  2.0791e-05, -1.3063e-05, -3.9283e-06,\n",
      "        -1.0901e-05, -4.8251e-05, -1.6542e-05,  3.8408e-06, -1.4834e-05,\n",
      "         6.4078e-06,  1.7828e-06, -2.0738e-05, -1.8394e-05,  1.9318e-05,\n",
      "        -1.0414e-05, -5.5441e-06, -8.1529e-06,  1.2198e-06, -5.3851e-07,\n",
      "        -1.8823e-06, -1.1292e-05, -8.4057e-06,  4.4713e-06,  2.4680e-06,\n",
      "         9.3794e-06, -1.4007e-05, -5.6931e-06, -4.0454e-05,  4.3851e-05,\n",
      "         6.1233e-06,  3.5969e-05, -1.5511e-05,  5.9517e-06,  1.2169e-04,\n",
      "         4.2475e-05,  3.3735e-05, -5.6646e-06,  1.3964e-04,  8.5922e-06,\n",
      "         3.6636e-06,  1.3441e-05, -2.0611e-05, -3.0478e-05, -5.1082e-06,\n",
      "        -9.6205e-06,  1.6288e-05, -2.6611e-05,  6.7639e-05, -2.8135e-05,\n",
      "        -3.8266e-05, -3.1154e-05,  9.8901e-07,  5.3889e-05,  2.6624e-05,\n",
      "        -6.8203e-05, -1.2075e-04,  9.4322e-06, -9.8882e-06, -1.7912e-05,\n",
      "         3.5107e-06, -1.6035e-05, -1.3541e-05, -1.1440e-06, -4.4940e-05,\n",
      "         1.4290e-05, -1.5295e-05, -1.5501e-05, -3.3103e-05,  8.9247e-05,\n",
      "         7.7616e-05, -1.0010e-05, -2.0305e-06, -4.7462e-06, -1.2766e-04,\n",
      "        -8.0178e-06,  3.4036e-06,  5.7867e-06,  8.1794e-06, -9.2285e-06,\n",
      "         1.1070e-06, -1.4535e-05,  5.6030e-05,  1.8184e-05,  1.2684e-05,\n",
      "        -3.6469e-05, -1.3368e-05, -3.7054e-04,  4.7672e-05,  8.1853e-06,\n",
      "        -4.7665e-05,  9.2846e-05,  1.9834e-05, -4.4049e-06, -1.4272e-07,\n",
      "         8.7826e-06, -1.2901e-05, -1.6625e-05, -2.2718e-06,  3.1114e-05,\n",
      "         2.0147e-05,  7.3631e-06, -1.2900e-05,  1.1390e-05,  9.0955e-06,\n",
      "         3.6225e-05, -1.8709e-05,  2.3320e-05,  1.0087e-06, -1.3740e-05,\n",
      "         4.4956e-05, -4.7125e-06, -4.0257e-04, -1.7307e-05, -1.6551e-05,\n",
      "         4.5725e-05, -1.9404e-05,  4.6613e-05,  6.1360e-06,  5.8740e-05,\n",
      "        -1.3411e-06,  3.2362e-05, -2.2883e-05, -3.7427e-05, -3.4377e-05,\n",
      "         5.6606e-04,  8.9922e-06, -1.8352e-05, -1.8087e-05, -1.2084e-05,\n",
      "         1.8915e-06, -2.7572e-05, -4.3583e-05,  5.3643e-04, -1.5114e-05,\n",
      "         1.9102e-05, -6.3662e-07,  7.2328e-06, -1.0887e-05,  1.4136e-05,\n",
      "        -2.3688e-05, -2.2531e-05, -2.3523e-05, -1.1884e-04,  1.8460e-05,\n",
      "        -1.7613e-05, -2.2992e-05,  5.1504e-05, -1.9987e-05, -9.7811e-05,\n",
      "        -2.4212e-05, -1.3219e-05,  8.9993e-06, -2.5880e-05, -9.4469e-06,\n",
      "         8.8795e-06,  4.4109e-05,  1.5214e-04, -3.3033e-07, -2.9437e-05,\n",
      "         3.6250e-05, -3.9730e-05,  2.3890e-04,  2.6127e-05, -4.9313e-05,\n",
      "         8.9386e-05,  2.3116e-05,  1.8861e-06,  2.8282e-05, -1.8039e-05,\n",
      "        -1.9093e-05, -1.2735e-05, -8.7705e-06,  1.1907e-05, -3.1451e-05,\n",
      "        -7.2752e-06,  9.7065e-05,  2.5405e-05,  1.8190e-05,  5.8336e-06,\n",
      "         9.7113e-06, -1.5032e-05,  2.5965e-05,  2.1651e-06, -2.0602e-05,\n",
      "         5.4472e-05, -4.1762e-05,  7.3128e-06, -3.7616e-05, -5.3985e-06,\n",
      "         1.5269e-05, -4.3047e-06, -3.4032e-05,  4.3122e-06, -2.1645e-05,\n",
      "         7.0304e-06, -1.1588e-05, -8.3923e-06,  2.7504e-06, -1.7545e-07,\n",
      "        -3.0760e-06, -2.3227e-05, -4.8807e-05, -2.8904e-05, -5.5143e-05,\n",
      "         1.3771e-05, -3.9055e-05, -1.5586e-05,  6.6485e-05,  7.7931e-06,\n",
      "         2.8930e-06, -2.7988e-06,  4.4006e-05, -3.7710e-05, -8.5345e-07,\n",
      "        -1.8265e-05,  1.9092e-04, -4.6124e-05, -4.2579e-05,  4.6621e-04,\n",
      "        -4.4342e-06, -2.7422e-05,  4.2942e-06,  3.8952e-06, -2.3523e-05,\n",
      "        -8.7192e-06, -3.4426e-05,  6.8796e-07, -2.1650e-05, -5.9742e-06,\n",
      "         1.1675e-05,  1.3853e-05, -9.7511e-06,  1.6634e-05, -1.6007e-05,\n",
      "         6.5734e-05, -1.7119e-05,  2.7955e-04, -1.0354e-05,  3.8515e-06,\n",
      "        -8.3261e-06, -1.8448e-05,  5.8793e-04,  1.2295e-03, -3.9355e-04,\n",
      "        -3.6408e-04, -3.6279e-05, -1.7054e-04,  1.9273e-04, -4.6065e-04,\n",
      "         1.7174e-03, -8.5749e-04, -4.6516e-05, -1.9992e-04,  2.8590e-04,\n",
      "        -2.7377e-05, -4.4514e-04, -2.1732e-04, -1.5953e-04, -7.6496e-06,\n",
      "         1.7465e-05,  5.6219e-04,  7.5752e-04, -1.4239e-04, -2.8395e-04,\n",
      "         1.3071e-05,  1.0395e-05,  2.6471e-04, -5.9976e-05,  7.6969e-05,\n",
      "         8.8389e-04,  2.0446e-03,  1.6952e-04,  1.8597e-04,  2.8382e-06,\n",
      "        -2.9961e-05, -2.9347e-04,  3.1646e-05,  4.3845e-05,  3.4000e-05,\n",
      "         1.5423e-05, -1.1371e-04,  1.2385e-04, -1.5836e-05, -7.0597e-06,\n",
      "         1.1610e-05, -1.0793e-04, -1.0389e-04, -1.0342e-04,  2.3572e-04,\n",
      "         7.4253e-04, -6.4952e-04, -5.7230e-05,  4.9605e-05,  3.5674e-04,\n",
      "        -1.1852e-04,  4.7629e-04,  1.9357e-04,  3.1397e-06, -1.1688e-04,\n",
      "        -1.3396e-04,  8.0210e-05, -4.8786e-04, -2.7494e-05,  4.1516e-04,\n",
      "         7.0665e-04, -4.3115e-06,  5.0491e-04,  3.4589e-05,  4.3583e-04,\n",
      "        -8.7129e-04, -1.1350e-03,  1.7782e-03,  1.6186e-05,  9.8393e-05,\n",
      "         1.5237e-03,  1.0522e-04, -3.5556e-05, -6.6953e-05,  2.5891e-05,\n",
      "         2.4157e-05,  2.1796e-05, -3.0594e-04, -1.7709e-04,  7.2302e-06,\n",
      "         3.8227e-04, -6.6003e-04, -1.4119e-05,  2.4967e-04, -3.8511e-04,\n",
      "        -6.5886e-04, -1.2935e-05, -9.9513e-04, -3.6861e-04,  4.9401e-04,\n",
      "        -2.6560e-04,  6.1722e-04, -3.0867e-04,  1.6216e-04,  8.0750e-05,\n",
      "         1.6566e-05,  4.3146e-03,  1.1750e-05,  2.0067e-03, -4.0113e-04,\n",
      "        -6.7866e-05,  3.0505e-05,  5.1441e-04, -9.5084e-05, -9.1345e-05,\n",
      "        -1.3085e-05, -2.5271e-05, -1.9905e-05,  5.0247e-04,  2.0958e-03,\n",
      "        -1.8148e-04,  6.6638e-04, -1.6676e-06, -8.2090e-05, -1.2036e-04,\n",
      "        -2.4660e-04, -3.9634e-05,  4.6021e-05, -4.3416e-05,  1.9897e-05,\n",
      "         5.6764e-04,  5.2828e-05,  1.9467e-03, -1.7217e-04, -2.5008e-05,\n",
      "        -3.0432e-04,  2.3812e-04,  1.1334e-05,  8.5874e-04,  2.6719e-07,\n",
      "         4.4122e-05, -5.0305e-04, -8.2589e-05,  4.8976e-03,  2.6455e-05,\n",
      "         2.2120e-03,  9.1662e-04,  3.8936e-05,  1.4018e-03,  8.2187e-05,\n",
      "        -1.1527e-04, -3.3419e-05, -8.3752e-05, -1.6292e-04,  3.6862e-03,\n",
      "        -9.4792e-08, -1.2765e-04,  6.7167e-04,  1.8862e-05, -2.0404e-04,\n",
      "         2.0411e-04, -3.6590e-04, -4.2594e-04,  2.3194e-04, -4.0925e-04,\n",
      "         8.4455e-05,  7.7808e-05,  4.0347e-06,  8.6827e-04,  6.1862e-04,\n",
      "        -3.9078e-04,  5.8439e-05, -1.8609e-05,  2.6422e-06, -1.1817e-05,\n",
      "        -6.5577e-05,  3.3912e-04,  5.0011e-04,  2.1796e-03, -4.7764e-06,\n",
      "        -6.9971e-05, -1.3561e-04, -3.2455e-04, -4.7646e-04, -1.4334e-05,\n",
      "         2.2859e-03,  9.4962e-04,  5.4352e-05, -1.6623e-05, -9.1089e-05,\n",
      "         1.3631e-04, -1.0683e-06,  1.2822e-05, -1.2589e-04, -1.6874e-05,\n",
      "         4.8645e-04,  5.8831e-04, -2.1525e-04,  1.7046e-04,  2.2147e-04,\n",
      "         9.7029e-05, -1.0835e-04,  2.1833e-05,  5.3221e-04,  1.7423e-05,\n",
      "         5.6180e-06, -7.1509e-06,  1.3560e-04,  8.4806e-06, -4.2440e-05,\n",
      "         2.1661e-05,  1.3891e-03, -5.1648e-05, -4.9862e-04,  9.8611e-05,\n",
      "         3.8533e-04, -1.1975e-06, -6.0577e-05,  5.3252e-05, -2.8512e-06,\n",
      "         9.3808e-05, -3.6528e-06,  3.4282e-05,  1.1277e-03, -5.1905e-05,\n",
      "        -9.7135e-05, -6.4639e-05,  1.5596e-05, -1.0423e-04, -3.8127e-04,\n",
      "         8.8865e-05,  6.7241e-04,  8.9034e-06, -1.3456e-04,  5.6449e-04,\n",
      "        -4.1174e-04, -1.2648e-03, -4.1823e-05,  8.4879e-04,  2.2028e-04,\n",
      "         5.4514e-06,  6.7427e-04, -3.4584e-05, -1.1144e-04,  7.4218e-06,\n",
      "         4.3218e-04, -9.0047e-05,  1.1798e-03, -2.1201e-04, -2.8882e-05,\n",
      "        -1.1304e-04,  4.0252e-04, -4.2353e-05,  1.2846e-05, -1.7361e-04,\n",
      "         3.2679e-04, -1.9586e-04,  3.8608e-04, -2.9757e-04,  7.0321e-04,\n",
      "         1.7172e-04, -3.7375e-05,  5.2956e-05, -1.2155e-04, -2.4857e-04,\n",
      "         1.1366e-04,  2.5622e-06,  1.2285e-05, -2.1707e-06,  6.5088e-05,\n",
      "        -1.2974e-04, -7.7789e-05,  6.8711e-05,  5.0091e-05,  3.2428e-05,\n",
      "        -1.7279e-05, -2.3037e-05,  1.4563e-05,  4.1323e-05, -6.5235e-05,\n",
      "        -5.4987e-05,  8.2814e-06,  6.8067e-05, -1.9648e-04, -5.1906e-05,\n",
      "         3.9000e-05, -6.4889e-05,  1.3147e-05, -1.2336e-05,  7.2132e-05,\n",
      "         9.7165e-05, -7.6652e-05, -4.3140e-05,  2.1738e-05,  3.2642e-05,\n",
      "         5.4908e-06,  3.7192e-06,  2.0413e-05, -4.8572e-05, -3.8683e-05,\n",
      "         1.1001e-06,  3.6097e-05,  3.7339e-05,  3.5796e-05, -2.1602e-06,\n",
      "        -7.9235e-05,  3.2610e-04,  2.0481e-05, -7.4513e-05, -8.3441e-05,\n",
      "        -2.3809e-05,  3.6561e-04,  1.1275e-04, -1.3375e-04, -2.1045e-05,\n",
      "        -2.3839e-04,  3.7334e-05,  1.6194e-05, -3.0493e-05,  9.7197e-05,\n",
      "        -1.2775e-04, -1.1088e-05, -1.3966e-05, -3.4792e-05,  5.3161e-05,\n",
      "        -6.8972e-05,  8.6215e-05,  1.4515e-04,  1.2661e-05, -3.9944e-06,\n",
      "         1.6437e-04,  2.1424e-04, -3.2739e-04, -1.1339e-04,  5.1997e-05,\n",
      "         2.3421e-05,  3.6206e-05,  9.0498e-06, -4.9478e-05,  3.9466e-05,\n",
      "        -2.3042e-06,  1.3254e-04,  1.1668e-05, -4.2375e-05,  5.3463e-05,\n",
      "        -8.5463e-05, -1.5037e-04,  1.2441e-05,  1.7998e-05,  1.3075e-05,\n",
      "        -2.2099e-05,  1.3097e-04, -4.7252e-05,  7.7382e-04,  4.1920e-05,\n",
      "         1.8586e-05,  2.1713e-05, -3.5467e-05,  3.6288e-05, -1.2472e-04,\n",
      "         1.5533e-04,  3.0395e-05,  6.2306e-04,  4.1918e-05, -7.9542e-04,\n",
      "         1.8843e-05, -3.8330e-05,  8.2523e-05, -9.2814e-05, -4.9761e-05,\n",
      "        -2.5028e-05, -4.8523e-07, -2.2202e-05, -4.5309e-05,  3.9813e-05,\n",
      "        -5.4868e-06,  6.6802e-05, -8.6261e-05, -1.6258e-03, -3.8974e-05,\n",
      "         1.1704e-05,  4.9727e-05,  1.0749e-05,  4.3541e-05,  4.3056e-05,\n",
      "         7.2345e-06, -5.2849e-05,  1.1856e-04, -1.5143e-06, -7.4273e-04,\n",
      "         5.4794e-05, -6.0126e-05, -1.4670e-04, -5.1061e-04,  1.0938e-04,\n",
      "        -2.6995e-04,  1.4964e-04, -6.4554e-06, -7.7348e-05,  2.0552e-04,\n",
      "        -1.4872e-04,  7.4231e-05,  3.3891e-04,  1.0380e-05,  2.9448e-05,\n",
      "        -9.5093e-05, -2.6404e-05, -1.1533e-05,  7.6524e-05, -2.7529e-04,\n",
      "         4.8402e-04, -8.1925e-05, -4.8595e-05,  4.4592e-05,  2.6410e-05,\n",
      "         3.1720e-05, -3.5021e-05, -7.3511e-05,  1.5508e-05, -7.7244e-05,\n",
      "         1.8655e-04, -3.5552e-05, -1.3073e-05,  7.8081e-05,  4.7020e-04,\n",
      "        -5.4537e-05, -8.0909e-04, -4.2954e-05,  3.9191e-05, -2.4341e-05,\n",
      "        -6.7979e-05, -3.9324e-05, -2.2426e-05,  1.2490e-04,  1.9440e-04,\n",
      "        -1.2510e-06,  4.5629e-05, -1.3802e-04, -6.0296e-05,  4.5586e-04,\n",
      "        -6.2835e-05,  1.8802e-04,  1.3720e-04, -1.5012e-04,  8.8593e-06,\n",
      "        -9.3929e-05,  3.0184e-05, -1.0798e-04, -3.8329e-05, -7.0170e-04,\n",
      "         5.7497e-05,  1.0556e-04, -3.0497e-05,  1.7241e-04, -2.4092e-05,\n",
      "        -1.1898e-05, -3.7447e-06,  3.5613e-05,  3.5618e-05,  4.0031e-05,\n",
      "        -1.1682e-05,  6.4207e-05, -8.8299e-05, -8.9376e-05,  3.8898e-05,\n",
      "         8.9770e-05,  9.1480e-06, -7.4936e-05,  3.6866e-05, -9.4067e-05,\n",
      "         1.5245e-03,  7.1152e-05, -3.6423e-05,  3.8846e-05,  3.6999e-05,\n",
      "        -1.0963e-05, -1.1875e-05,  9.3964e-06,  6.0371e-05, -1.6374e-04,\n",
      "        -7.8630e-05,  1.2079e-04, -4.1228e-05, -1.9792e-04,  6.2553e-05,\n",
      "        -2.6947e-04,  1.9942e-05, -1.9404e-05, -1.1040e-05, -1.3539e-04,\n",
      "        -5.3540e-05, -4.7154e-05, -2.6250e-04,  9.2630e-05, -9.4601e-05,\n",
      "        -1.4098e-04,  5.4675e-06, -2.2160e-05,  1.2144e-04,  1.0394e-05,\n",
      "        -4.1761e-05,  2.0580e-04, -1.5768e-06, -6.9811e-05,  5.0703e-06,\n",
      "         4.8028e-05, -7.8178e-06, -4.0421e-04, -2.6370e-05, -4.4024e-05,\n",
      "         6.3023e-05, -4.3257e-05,  2.5729e-04,  3.6623e-05, -3.7258e-05,\n",
      "        -3.1095e-05, -4.4641e-06, -2.2743e-05])\n",
      "tensor([[ 4.7419e-06, -1.3870e-06, -1.2579e-06,  ...,  4.5460e-06,\n",
      "         -6.5051e-06,  8.0584e-06],\n",
      "        [-1.2305e-02,  1.2692e-02,  1.0367e-02,  ..., -1.2654e-02,\n",
      "          1.1907e-02, -3.1207e-02],\n",
      "        [ 7.4275e-03, -1.3538e-02, -1.3253e-02,  ...,  5.1461e-03,\n",
      "         -8.1569e-03,  1.9376e-02],\n",
      "        ...,\n",
      "        [ 7.3798e-06, -3.6148e-06, -3.6797e-06,  ...,  7.7631e-06,\n",
      "         -1.2012e-05,  1.5669e-05],\n",
      "        [ 5.5044e-06, -2.1960e-06, -2.1579e-06,  ...,  5.8161e-06,\n",
      "         -9.0051e-06,  1.1191e-05],\n",
      "        [ 7.6761e-06, -3.1656e-06, -3.0531e-06,  ...,  7.7050e-06,\n",
      "         -1.1554e-05,  1.4751e-05]])\n",
      "tensor([ 8.0984e-06, -1.2101e-02,  7.5527e-03,  7.4314e-05,  2.0254e-05,\n",
      "         1.3539e-05,  9.8913e-06,  8.6270e-05,  1.2525e-03,  2.4350e-05,\n",
      "         2.2271e-05,  1.4192e-05,  4.3663e-05,  2.7911e-05, -2.1288e-03,\n",
      "         7.7306e-04,  2.5366e-04,  1.1420e-03, -2.5072e-03,  1.0255e-03,\n",
      "         7.0648e-04,  6.0897e-04,  6.0125e-04,  6.0894e-04,  5.8680e-04,\n",
      "        -1.6039e-03,  1.2018e-03,  5.2999e-05,  1.8900e-05,  3.5508e-05,\n",
      "         1.1822e-05,  1.1852e-05,  1.6824e-05, -1.7425e-02,  1.6811e-03,\n",
      "         3.9240e-04, -6.7827e-03, -1.6865e-03, -3.5519e-03, -2.1494e-04,\n",
      "         2.6590e-03, -2.1961e-03, -1.8809e-03,  4.5032e-03, -5.5700e-03,\n",
      "        -1.7676e-02,  4.4926e-03,  1.4613e-02,  8.1259e-03, -1.0610e-03,\n",
      "         2.8052e-03, -8.0158e-03,  1.6692e-02, -1.0829e-02,  1.1103e-03,\n",
      "         5.2060e-03, -6.6047e-05,  3.0248e-03,  5.5028e-03,  3.6836e-05,\n",
      "         1.0279e-05,  1.0546e-05,  8.6503e-06,  1.1215e-05,  8.9017e-06,\n",
      "         1.9041e-05,  1.9827e-05,  2.3415e-05,  3.7151e-05,  4.7837e-04,\n",
      "         7.2981e-05,  5.6283e-05,  1.2045e-04,  3.5391e-05,  3.9828e-05,\n",
      "         8.4941e-05,  1.0369e-04,  7.6206e-04,  2.8307e-05,  8.4395e-05,\n",
      "         1.4597e-05,  2.7090e-04,  3.4540e-05,  4.3690e-05,  3.1454e-05,\n",
      "         5.6213e-05,  2.8412e-05,  3.1119e-04,  4.8301e-05,  1.9664e-05,\n",
      "         2.6859e-04,  4.6039e-05,  9.7311e-06,  7.6452e-05,  1.3578e-05,\n",
      "         2.2476e-04,  2.8735e-05,  1.2014e-05,  1.7759e-05,  4.6502e-05,\n",
      "         1.1787e-04,  3.0513e-05,  1.9735e-04,  1.1467e-04,  9.0362e-06,\n",
      "         2.1731e-05,  1.7029e-05,  3.8932e-05,  3.8868e-05,  2.4253e-05,\n",
      "         3.4883e-05,  1.1651e-05,  1.8207e-05,  2.8383e-05,  6.8604e-05,\n",
      "         1.0289e-05,  1.3310e-05,  1.6619e-05,  1.5701e-04,  6.4175e-05,\n",
      "         1.1368e-05,  9.7282e-06,  1.2051e-05,  1.2275e-04,  2.0397e-05,\n",
      "         4.4613e-05,  1.4015e-05,  2.6037e-05,  3.5800e-05,  3.9101e-05,\n",
      "         1.7119e-04,  1.2207e-05,  1.0699e-05,  5.4048e-05,  1.1271e-05,\n",
      "         1.5305e-05,  8.1548e-06,  1.1519e-05,  1.6427e-05,  2.9877e-05,\n",
      "         8.1634e-05,  1.2092e-05,  1.0816e-05,  7.5303e-05,  4.2252e-05,\n",
      "         1.9310e-05,  1.2130e-05,  1.2299e-05,  1.7469e-05,  4.3967e-05,\n",
      "         9.9260e-06,  1.6857e-05,  8.9906e-06,  1.4285e-05,  1.3768e-05,\n",
      "         1.5451e-05,  1.0402e-05,  1.3706e-05,  8.6015e-06,  1.1528e-05,\n",
      "         1.1365e-05,  1.4042e-05,  1.4081e-05,  2.1261e-05,  1.0793e-05,\n",
      "         1.8794e-05,  1.2073e-05,  1.2349e-05,  1.0191e-05,  1.3497e-05,\n",
      "         1.3250e-05,  1.5809e-05,  1.9893e-05,  1.7152e-05,  5.8114e-06,\n",
      "         1.4183e-05,  8.5695e-06,  7.8648e-06,  1.0204e-05,  7.4282e-06,\n",
      "         9.9003e-06,  1.4972e-05,  1.0663e-05,  7.9923e-06,  9.4319e-06,\n",
      "         8.7778e-06,  8.7217e-06,  1.4026e-05,  8.3940e-06,  2.5619e-05,\n",
      "         2.9244e-05,  1.2886e-05,  2.5527e-05,  1.7796e-05,  3.0757e-05,\n",
      "         1.9761e-05,  2.2127e-05,  2.5568e-05,  4.6859e-05,  8.9160e-06,\n",
      "         2.7406e-05,  1.9623e-05,  1.4248e-05,  1.9303e-05,  2.4145e-05,\n",
      "         1.7623e-05,  2.0418e-05,  9.3966e-06,  1.2958e-05,  8.9161e-06,\n",
      "         2.6431e-05,  3.6548e-05,  2.0896e-05,  2.0343e-05,  1.4532e-05,\n",
      "         1.2616e-05,  1.9144e-05,  1.2108e-05,  1.2450e-05,  1.2444e-05,\n",
      "         1.1928e-05,  7.7248e-06,  9.0239e-06,  1.1073e-05,  1.3392e-05,\n",
      "         1.3426e-05,  7.1971e-06,  1.2319e-05,  1.6361e-05,  1.6320e-05,\n",
      "         1.1497e-05,  1.1814e-05,  1.9341e-05,  1.2946e-05,  1.5577e-05,\n",
      "         1.4681e-05,  1.1336e-05,  1.2551e-05,  1.6684e-05,  1.2073e-05,\n",
      "         1.4933e-05,  1.0103e-05,  1.2650e-05,  7.4894e-06,  1.3972e-04,\n",
      "         1.5544e-05,  1.4148e-05,  1.8919e-05,  9.0452e-06,  8.9168e-06,\n",
      "         1.0221e-05,  1.0720e-05,  1.4589e-05,  9.0337e-06,  8.1919e-06,\n",
      "         7.6224e-06,  9.5761e-06,  1.1667e-05,  1.1957e-05,  1.2081e-05,\n",
      "         1.3904e-05,  1.0667e-05,  1.0282e-05,  1.0344e-05,  1.3994e-05,\n",
      "         1.4477e-05,  1.4863e-05,  1.1048e-05,  1.0623e-05,  1.3687e-05,\n",
      "         1.0770e-05,  8.4483e-06,  9.7537e-06,  1.1933e-05,  1.5289e-05,\n",
      "         1.1419e-05,  9.1351e-06,  1.0968e-05,  8.0149e-06,  1.3176e-05,\n",
      "         1.4674e-05,  1.1181e-05,  1.4183e-05])\n"
     ]
    }
   ],
   "source": [
    "for w in model.parameters():\n",
    "    print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0077,  0.0262, -0.0100, -0.0444,  0.0125,  0.0249, -0.0399,  0.0476,\n",
       "        -0.0599, -0.0452, -0.0364, -0.0570, -0.0045,  0.0164, -0.0283, -0.0012,\n",
       "         0.0414,  0.0353, -0.0030,  0.0489, -0.0177,  0.0022,  0.0206, -0.0415,\n",
       "         0.0346, -0.0457,  0.0193,  0.0104,  0.0265,  0.0476, -0.0270,  0.0510,\n",
       "        -0.0167, -0.0010, -0.0539, -0.0222, -0.0441, -0.0392,  0.0111, -0.0099,\n",
       "         0.0059, -0.0147, -0.0204,  0.0608,  0.0361, -0.0357,  0.0311,  0.0143,\n",
       "        -0.0163,  0.0033,  0.0606, -0.0305,  0.0484,  0.0555,  0.0489, -0.0519,\n",
       "         0.0425,  0.0177, -0.0042, -0.0591, -0.0094, -0.0341,  0.0052,  0.0075,\n",
       "         0.0317,  0.0242, -0.0674, -0.0396,  0.0061, -0.0687,  0.0210, -0.0092,\n",
       "         0.0480, -0.0640, -0.0154, -0.0448, -0.0592, -0.0021, -0.0558, -0.0055,\n",
       "         0.0337,  0.0102,  0.0361, -0.0108, -0.0619, -0.0600, -0.0586, -0.0268,\n",
       "        -0.0384, -0.0230,  0.0058, -0.0067,  0.0114,  0.0361, -0.0121, -0.0419,\n",
       "        -0.0704,  0.0392,  0.0018,  0.0144, -0.0351, -0.0353,  0.0021,  0.0555,\n",
       "        -0.0698, -0.0100, -0.0616, -0.0522, -0.0338, -0.0538,  0.0487,  0.0311,\n",
       "        -0.0616,  0.0146,  0.0295, -0.0065,  0.0486,  0.0104, -0.0309,  0.0119,\n",
       "        -0.0572, -0.0688,  0.0394,  0.0518, -0.0098,  0.0345, -0.0229,  0.0143,\n",
       "        -0.0610, -0.0214, -0.0226, -0.0482,  0.0181,  0.0095, -0.0178,  0.0202,\n",
       "        -0.0496,  0.0455,  0.0411, -0.0426, -0.0235, -0.0180, -0.0379, -0.0027,\n",
       "         0.0192, -0.0557, -0.0382,  0.0351,  0.0009, -0.0458, -0.0644, -0.0573,\n",
       "        -0.0466, -0.0125,  0.0380, -0.0162,  0.0475,  0.0390,  0.0502,  0.0091,\n",
       "        -0.0083, -0.0156,  0.0058, -0.0631,  0.0130,  0.0014,  0.0316, -0.0209,\n",
       "        -0.0213,  0.0174,  0.0233,  0.0269,  0.0332,  0.0046, -0.0077, -0.0023,\n",
       "         0.0374, -0.0372, -0.0338, -0.0613,  0.0008,  0.0439, -0.0599, -0.0529,\n",
       "        -0.0289, -0.0678, -0.0337,  0.0264, -0.0570,  0.0249, -0.0534,  0.0287,\n",
       "         0.0163, -0.0198, -0.0096, -0.0558,  0.0205,  0.0454,  0.0524,  0.0371,\n",
       "         0.0236, -0.0315,  0.0386,  0.0318, -0.0296, -0.0139,  0.0230,  0.0095,\n",
       "        -0.0641,  0.0422, -0.0592,  0.0331,  0.0092,  0.0059,  0.0273, -0.0269,\n",
       "        -0.0014,  0.0365,  0.0318, -0.0483, -0.0144, -0.0108,  0.0050, -0.0190,\n",
       "        -0.0191,  0.0349, -0.0699,  0.0099,  0.0253, -0.0398,  0.0355, -0.0324,\n",
       "         0.0354, -0.0306, -0.0641,  0.0392,  0.0506,  0.0148,  0.0522,  0.0106,\n",
       "        -0.0386,  0.0415,  0.0137, -0.0496,  0.0345,  0.0487, -0.0572,  0.0485,\n",
       "        -0.0175, -0.0670, -0.0186, -0.0644, -0.0317, -0.0051, -0.0214,  0.0086,\n",
       "        -0.0348,  0.0335,  0.0533, -0.0223, -0.0328,  0.0003, -0.0622, -0.0019,\n",
       "         0.0138,  0.0334,  0.0424, -0.0157, -0.0595,  0.0418, -0.0052,  0.0484,\n",
       "         0.0082, -0.0123,  0.0397, -0.0413,  0.0525,  0.0367, -0.0344, -0.0402,\n",
       "         0.0056,  0.0167,  0.0498], requires_grad=True)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How to interpret the plots:__\n",
    "\n",
    "* __Train loss__ - that's your model's crossentropy over minibatches. It should go down steadily. Most importantly, it shouldn't be NaN :)\n",
    "* __Val score distribution__ - distribution of translation edit distance (score) within batch. It should move to the left over time.\n",
    "* __Val score / training time__ - it's your current mean edit distance. This plot is much whimsier than loss, but make sure it goes below 8 by 2500 steps. \n",
    "\n",
    "If it doesn't, first try to re-create both model and opt. You may have changed it's weight too much while debugging. If that doesn't help, it's debugging time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in train_words[:10]:\n",
    "    print(\"%s -> %s\" % (word, translate([word])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = []\n",
    "for start_i in trange(0, len(test_words), 32):\n",
    "    batch_words = test_words[start_i:start_i+32]\n",
    "    batch_trans = translate(batch_words)\n",
    "    distances = list(map(get_distance, batch_words, batch_trans))\n",
    "    test_scores.extend(distances)\n",
    "\n",
    "print(\"Supervised test score:\", np.mean(test_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-critical policy gradient (2 points)\n",
    "\n",
    "In this section you'll implement algorithm called self-critical sequence training (here's an [article](https://arxiv.org/abs/1612.00563)).\n",
    "\n",
    "The algorithm is a vanilla policy gradient with a special baseline. \n",
    "\n",
    "$$ \\nabla J = E_{x \\sim p(s)} E_{y \\sim \\pi(y|x)} \\nabla log \\pi(y|x) \\cdot (R(x,y) - b(x)) $$\n",
    "\n",
    "Here reward R(x,y) is a __negative levenshtein distance__ (since we minimize it). The baseline __b(x)__ represents how well model fares on word __x__.\n",
    "\n",
    "In practice, this means that we compute baseline as a score of greedy translation, $b(x) = R(x,y_{greedy}(x)) $.\n",
    "\n",
    "![img](https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/scheme.png)\n",
    "\n",
    "\n",
    "Luckily, we already obtained the required outputs: `model.greedy_translations, model.greedy_mask` and we only need to compute levenshtein using `compute_levenshtein` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(input_sequence, translations):\n",
    "    \"\"\" computes sample-wise reward given token ids for inputs and translations \"\"\"\n",
    "    distances = list(map(get_distance,\n",
    "                         inp_voc.to_lines(input_sequence.data.numpy()),\n",
    "                         out_voc.to_lines(translations.data.numpy())))\n",
    "    # use negative levenshtein distance so that larger reward means better policy\n",
    "    return - torch.tensor(distances, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scst_objective_on_batch(input_sequence, max_len=MAX_OUTPUT_LENGTH):\n",
    "    \"\"\" Compute pseudo-loss for policy gradient given a batch of sources \"\"\"\n",
    "    input_sequence = torch.tensor(inp_voc.to_matrix(input_sequence), dtype=torch.int64)\n",
    "\n",
    "    # use model to __sample__ symbolic translations given input_sequence\n",
    "    sample_translations, sample_logp = <YOUR CODE>\n",
    "    # use model to __greedy__ symbolic translations given input_sequence\n",
    "    greedy_translations, greedy_logp = <YOUR CODE>\n",
    "\n",
    "    # compute rewards and advantage\n",
    "    rewards = compute_reward(input_sequence, sample_translations)\n",
    "    baseline = <YOUR CODE: compute __negative__ levenshtein for greedy mode>\n",
    "\n",
    "    # compute advantage using rewards and baseline\n",
    "    advantage =  <YOUR CODE>\n",
    "\n",
    "    # compute log_pi(a_t|s_t), shape = [batch, seq_length]\n",
    "    logp_sample = <YOUR CODE>\n",
    "    \n",
    "    # ^-- hint: look at how crossentropy is implemented in supervised learning loss above\n",
    "    # mind the sign - this one should not be multiplied by -1 :)\n",
    "\n",
    "    # policy gradient pseudo-loss. Gradient of J is exactly policy gradient.\n",
    "    J = logp_sample * advantage[:, None]\n",
    "\n",
    "    assert J.dim() == 2, \"please return elementwise objective, don't compute mean just yet\"\n",
    "\n",
    "    # average with mask\n",
    "    mask = infer_mask(sample_translations, out_voc.eos_ix)\n",
    "    loss = - torch.sum(J * mask) / torch.sum(mask)\n",
    "\n",
    "    # regularize with negative entropy. Don't forget the sign!\n",
    "    # note: for entropy you need probabilities for all tokens (sample_logp), not just logp_sample\n",
    "    entropy = <YOUR CODE: compute entropy matrix of shape[batch, seq_length], H = -sum(p*log_p), don't forget the sign!>\n",
    "    # hint: you can get sample probabilities from sample_logp using math :)\n",
    "\n",
    "    assert entropy.dim(\n",
    "    ) == 2, \"please make sure elementwise entropy is of shape [batch,time]\"\n",
    "\n",
    "    reg = - 0.01 * torch.sum(entropy * mask) / torch.sum(mask)\n",
    "\n",
    "    return loss + reg, torch.sum(entropy * mask) / torch.sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy gradient training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_history = [np.nan] * len(loss_history)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in trange(100000):\n",
    "    loss, ent = scst_objective_on_batch(\n",
    "        sample_batch(train_words, word_to_translation, 32)[0])  # [0] = only source sentence\n",
    "\n",
    "    # train with backprop\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "\n",
    "    loss_history.append(loss.item())\n",
    "    entropy_history.append(ent.item())\n",
    "\n",
    "    if (i+1) % REPORT_FREQ == 0:\n",
    "        clear_output(True)\n",
    "        current_scores = score(test_words)\n",
    "        editdist_history.append(current_scores.mean())\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(131)\n",
    "        plt.title('val score distribution')\n",
    "        plt.hist(current_scores, bins=20)\n",
    "        plt.subplot(132)\n",
    "        plt.title('val score / traning time')\n",
    "        plt.plot(editdist_history)\n",
    "        plt.grid()\n",
    "        plt.subplot(133)\n",
    "        plt.title('policy entropy / traning time')\n",
    "        plt.plot(entropy_history)\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        print(\"J=%.3f, mean score=%.3f\" %\n",
    "              (np.mean(loss_history[-10:]), np.mean(editdist_history[-10:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Debugging tips:__\n",
    "<img src=https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/do_something_scst.png width=400>\n",
    "\n",
    " * As usual, don't expect improvements right away, but in general the model should be able to show some positive changes by 5k steps.\n",
    " * Entropy is a good indicator of many problems. \n",
    "   * If it reaches zero, you may need greater entropy regularizer.\n",
    "   * If it has rapid changes time to time, you may need gradient clipping.\n",
    "   * If it oscillates up and down in an erratic manner... it's perfectly okay for entropy to do so. But it should decrease at the end.\n",
    "   \n",
    " * We don't show loss_history cuz it's uninformative for pseudo-losses in policy gradient. However, if something goes wrong you can check it to see if everything isn't a constant zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in train_words[:10]:\n",
    "    print(\"%s -> %s\" % (word, translate([word])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = []\n",
    "for start_i in trange(0, len(test_words), 32):\n",
    "    batch_words = test_words[start_i:start_i+32]\n",
    "    batch_trans = translate(batch_words)\n",
    "    distances = list(map(get_distance, batch_words, batch_trans))\n",
    "    test_scores.extend(distances)\n",
    "print(\"Supervised test score:\", np.mean(test_scores))\n",
    "\n",
    "# ^^ If you get Out Of MemoryError, please replace this with batched computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Make it actually work (5++ pts)\n",
    "\n",
    "In this section we want you to finally __restart with EASY_MODE=False__ and experiment to find a good model/curriculum for that task.\n",
    "\n",
    "We recommend you to start with the following architecture\n",
    "\n",
    "```\n",
    "encoder---decoder\n",
    "\n",
    "           P(y|h)\n",
    "             ^\n",
    " LSTM  ->   LSTM\n",
    "  ^          ^\n",
    " biLSTM  ->   LSTM\n",
    "  ^          ^\n",
    "input       y_prev\n",
    "```\n",
    "\n",
    "__Note:__ you can fit all 4 state tensors of both LSTMs into a in a single state - just assume that it contains, for example, [h0, c0, h1, c1] - pack it in encode and update in decode.\n",
    "\n",
    "\n",
    "Here are some cool ideas on what you can do then.\n",
    "\n",
    "__General tips & tricks:__\n",
    "* You will likely need to adjust pre-training time for such a network.\n",
    "* Supervised pre-training may benefit from clipping gradients somehow.\n",
    "* SCST may indulge a higher learning rate in some cases and changing entropy regularizer over time.\n",
    "* It's often useful to save pre-trained model parameters to not re-train it every time you want new policy gradient parameters. \n",
    "* When leaving training for nighttime, try setting REPORT_FREQ to a larger value (e.g. 500) not to waste time on it.\n",
    "\n",
    "__Formal criteria:__\n",
    "To get 5 points we want you to build an architecture that:\n",
    "* _doesn't consist of single GRU_\n",
    "* _works better_ than single GRU baseline. \n",
    "* We also want you to provide either learning curve or trained model, preferably both\n",
    "* ... and write a brief report or experiment log describing what you did and how it fared.\n",
    "\n",
    "### Attention\n",
    "There's more than one way to connect decoder to encoder\n",
    "  * __Vanilla:__ layer_i of encoder last state goes to layer_i of decoder initial state\n",
    "  * __Every tick:__ feed encoder last state _on every iteration_ of decoder.\n",
    "  * __Attention:__ allow decoder to \"peek\" at one (or several) positions of encoded sequence on every tick.\n",
    "  \n",
    "The most effective (and cool) of those is, of course, attention.\n",
    "You can read more about attention [in this nice blog post](https://distill.pub/2016/augmented-rnns/). The easiest way to begin is to use \"soft\" attention with \"additive\" or \"dot-product\" intermediate layers.\n",
    "\n",
    "__Tips__\n",
    "* Model usually generalizes better if you no longer allow decoder to see final encoder state\n",
    "* Once your model made it through several epochs, it is a good idea to visualize attention maps to understand what your model has actually learned\n",
    "\n",
    "* There's more stuff [here](https://github.com/yandexdataschool/Practical_RL/blob/master/week8_scst/bonus.ipynb)\n",
    "* If you opted for hard attention, we recommend [gumbel-softmax](https://blog.evjang.com/2016/11/tutorial-categorical-variational.html) instead of sampling. Also please make sure soft attention works fine before you switch to hard.\n",
    "\n",
    "### UREX\n",
    "* This is a way to improve exploration in policy-based settings. The main idea is that you find and upweight under-appreciated actions.\n",
    "* Here's [video](https://www.youtube.com/watch?v=fZNyHoXgV7M&feature=youtu.be&t=3444)\n",
    " and an [article](https://arxiv.org/abs/1611.09321).\n",
    "* You may want to reduce batch size 'cuz UREX requires you to sample multiple times per source sentence.\n",
    "* Once you got it working, try using experience replay with importance sampling instead of (in addition to) basic UREX.\n",
    "\n",
    "### Some additional ideas:\n",
    "* (advanced deep learning) It may be a good idea to first train on small phrases and then adapt to larger ones (a.k.a. training curriculum).\n",
    "* (advanced nlp) You may want to switch from raw utf8 to something like unicode or even syllables to make task easier.\n",
    "* (advanced nlp) Since hebrew words are written __with vowels omitted__, you may want to use a small Hebrew vowel markup dataset at `he-pron-wiktionary.txt`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not EASY_MODE, \"make sure you set EASY_MODE = False at the top of the notebook.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`[your report/log here or anywhere you please]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Contributions:__ This notebook is brought to you by\n",
    "* Yandex [MT team](https://tech.yandex.com/translate/)\n",
    "* Denis Mazur ([DeniskaMazur](https://github.com/DeniskaMazur)), Oleg Vasilev ([Omrigan](https://github.com/Omrigan/)), Dmitry Emelyanenko ([TixFeniks](https://github.com/tixfeniks)) and Fedor Ratnikov ([justheuristic](https://github.com/justheuristic/))\n",
    "* Dataset is parsed from [Wiktionary](https://en.wiktionary.org), which is under CC-BY-SA and GFDL licenses.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
