{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE in PyTorch\n",
    "\n",
    "Just like we did before for Q-learning, this time we'll design a PyTorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
    "\n",
    "Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting virtual X frame buffer: Xvfb.\r\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "    !touch .setup_complete\n",
    "\n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week5_policy_based/submit.py\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A caveat: with some versions of `pyglet`, the following cell may crash with `NameError: name 'base' is not defined`. The corresponding bug report is [here](https://github.com/pyglet/pyglet/issues/134). If you see this error, try restarting the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f937889ac10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# gym compatibility: unwrap TimeLimit\n",
    "if hasattr(env, '_max_episode_steps'):\n",
    "    env = env.env\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network for REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
    "\n",
    "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
    "We'll use softmax or log-softmax where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple neural network that predicts policy logits. \n",
    "# Keep it simple: CartPole isn't worth deep architectures.\n",
    "# model = nn.Sequential(\n",
    "#   <YOUR CODE: define a neural network that predicts policy logits>\n",
    "# )\n",
    "model = nn.Sequential()\n",
    "\n",
    "model.add_module('layer1', nn.Linear(state_dim[0], 256))\n",
    "model.add_module('layer2', nn.ReLU())\n",
    "model.add_module('layer3', nn.Linear(256, n_actions))\n",
    "\n",
    "softmax = nn.Softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: output value of this function is not a torch tensor, it's a numpy array.\n",
    "So, here gradient calculation is not needed.\n",
    "<br>\n",
    "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
    "to suppress gradient calculation.\n",
    "<br>\n",
    "Also, `.detach()` (or legacy `.data` property) can be used instead, but there is a difference:\n",
    "<br>\n",
    "With `.detach()` computational graph is built but then disconnected from a particular tensor,\n",
    "so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
    "<br>\n",
    "In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probs(states):\n",
    "    \"\"\" \n",
    "    Predict action probabilities given states.\n",
    "    :param states: numpy array of shape [batch, state_shape]\n",
    "    :returns: numpy array of shape [batch, n_actions]\n",
    "    \"\"\"\n",
    "    # convert states, compute logits, use softmax to get probability\n",
    "    states_ = torch.tensor(states, dtype=torch.float32)\n",
    "    preds = model(states_)\n",
    "    return softmax(preds).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_states = np.array([env.reset() for _ in range(5)])\n",
    "test_probas = predict_probs(test_states)\n",
    "assert isinstance(test_probas, np.ndarray), \\\n",
    "    \"you must return np array and not %s\" % type(test_probas)\n",
    "assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n",
    "    \"wrong output shape: %s\" % np.shape(test_probas)\n",
    "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play the game\n",
    "\n",
    "We can now use our newly built agent to play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(env, t_max=1000):\n",
    "    \"\"\" \n",
    "    Play a full session with REINFORCE agent.\n",
    "    Returns sequences of states, actions, and rewards.\n",
    "    \"\"\"\n",
    "    # arrays to record session\n",
    "    states, actions, rewards = [], [], []\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # action probabilities array aka pi(a|s)\n",
    "        action_probs = predict_probs(np.array([s]))[0]\n",
    "\n",
    "        # Sample action with given probabilities.\n",
    "        a = np.random.choice(n_actions, size=1, replace=False, p=action_probs)[0]\n",
    "        new_s, r, done, info = env.step(a)\n",
    "\n",
    "        # record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it\n",
    "states, actions, rewards = generate_session(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing cumulative rewards\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
    "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
    "&= r_t + \\gamma * G_{t + 1}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards,  # rewards at each step\n",
    "                           gamma=0.99  # discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    Take a list of immediate rewards r(s,a) for the whole session \n",
    "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
    "    \n",
    "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "\n",
    "    A simple way to compute cumulative rewards is to iterate from the last\n",
    "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
    "\n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    cum_r = []\n",
    "    prev_reward = 0.\n",
    "    for i, r in enumerate(rewards[::-1]):\n",
    "        cum_r.append(r + gamma * prev_reward)\n",
    "        prev_reward = cum_r[-1]\n",
    "        \n",
    "    return cum_r[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looks good!\n"
     ]
    }
   ],
   "source": [
    "get_cumulative_rewards(rewards)\n",
    "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
    "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
    "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
    "    [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
    "\n",
    "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
    "\n",
    "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
    "\n",
    "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y_tensor, ndims):\n",
    "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    y_one_hot = torch.zeros(\n",
    "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_one_hot(torch.tensor(1), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code: define optimizers\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
    "\n",
    "\n",
    "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # cast everything into torch tensors\n",
    "    states = torch.tensor(states, dtype=torch.float32)\n",
    "    actions = torch.tensor(actions, dtype=torch.int32)\n",
    "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
    "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
    "\n",
    "    # predict logits, probas and log-probas using an agent.\n",
    "    logits = model(states)\n",
    "    probs = nn.functional.softmax(logits, -1)\n",
    "    log_probs = nn.functional.log_softmax(logits, -1)\n",
    "\n",
    "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
    "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
    "\n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    log_probs_for_actions = torch.sum(\n",
    "        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n",
    "   \n",
    "    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n",
    "#     print(probs.shape, log_probs.shape)\n",
    "    entropy = torch.sum(-probs * log_probs, dim=-1)\n",
    "#     print(entropy.shape)\n",
    "    optimizer.zero_grad()\n",
    "#     print(log_probs_for_actions.shape, cumulative_returns.shape, entropy.shape)\n",
    "    loss = torch.mean(-log_probs_for_actions * cumulative_returns - entropy_coef * entropy)\n",
    "\n",
    "    # Gradient descent step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # technical: return session rewards to print them later\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:30.580\n",
      "mean reward:40.820\n",
      "mean reward:65.810\n",
      "mean reward:122.460\n",
      "mean reward:158.720\n",
      "mean reward:238.650\n",
      "mean reward:183.230\n",
      "mean reward:205.240\n",
      "mean reward:102.560\n",
      "mean reward:116.810\n",
      "mean reward:273.050\n",
      "mean reward:190.820\n",
      "mean reward:139.340\n",
      "mean reward:181.700\n",
      "mean reward:460.590\n",
      "mean reward:628.650\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n",
    "    \n",
    "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
    "    \n",
    "    if np.mean(rewards) > 500:\n",
    "        print(\"You Win!\")  # but you can train even further\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your average reward is 805.38 over 100 episodes\n",
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "from submit import submit_cartpole\n",
    "submit_cartpole(generate_session, '', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results & video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record sessions\n",
    "\n",
    "import gym.wrappers\n",
    "\n",
    "with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n",
    "    sessions = [generate_session(env_monitor) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"videos/openaigym.video.0.1638.video000064.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show video. This may not work in some setups. If it doesn't\n",
    "# work for you, you can download the videos and view them locally.\n",
    "\n",
    "from pathlib import Path\n",
    "from base64 import b64encode\n",
    "from IPython.display import HTML\n",
    "\n",
    "video_paths = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
    "video_path = video_paths[-1]  # You can also try other indices\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    # https://stackoverflow.com/a/57378660/1214547\n",
    "    with video_path.open('rb') as fp:\n",
    "        mp4 = fp.read()\n",
    "    data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
    "else:\n",
    "    data_url = str(video_path)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(data_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
